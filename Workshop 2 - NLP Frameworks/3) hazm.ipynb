{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src='https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png' width=150 height=150> <br>\n",
    "<font color=0F5298 size=6>\n",
    "Natural Language Processing<br>\n",
    "<font color=2565AE size=4>\n",
    "Computer Engineering Department<br>\n",
    "Spring 2025<br>\n",
    "<font color=3C99D size=4>\n",
    "Workshop 1 - NLP Frameworks - Hazm<br>\n",
    "<font color=696880 size=3>\n",
    "<a href='https://language.ml'>https://language.ml</a><br>\n",
    "info [AT] language [dot] ml"
   ],
   "id": "53ebb7727d583519"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📖 Part 1: Introduction\n",
    "\n",
    "## ❓ What is Hazm? ([Hazm Official Website](https://www.roshan-ai.ir/hazm/), [GitHub](https://github.com/sobhe/hazm))\n",
    "\n",
    "**Hazm** is a Python library for **Persian (Farsi) natural language processing**, offering tools tailored to Persian script and grammar. Key features include:\n",
    "\n",
    "- **Text normalization** (handling different Unicode variants, diacritics, etc.)\n",
    "- **Sentence & word tokenization** optimized for Persian\n",
    "- **Stemming** and **lemmatization** for Persian morphology\n",
    "- **POS tagging** and **dependency parsing** trained on Persian corpora\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ When to Use Hazm\n",
    "\n",
    "- You need **accurate Persian preprocessing** (normalization, tokenization).\n",
    "- You’re building **classical NLP pipelines** (stemming, POS tagging) for Persian text.\n",
    "- You require **dependency parses** of Persian sentences.\n",
    "- You want a **lightweight** library without heavy ML frameworks.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚫 When Not to Use Hazm\n",
    "\n",
    "- You need **deep learning** or **transformer-based models** for Persian (use 🤗 Hugging Face’s Persian models instead).\n",
    "- You require **multilingual pipelines**—Hazm is Persian-only.\n",
    "- You need **real-time, high-throughput** production systems at scale (consider spaCy + custom Persian models).\n",
    "- You want **entity linking** or **semantic role labeling**—Hazm focuses on core NLP tasks only."
   ],
   "id": "4806ddd3db8be1ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ⚙️ Installation & Setup",
   "id": "d66030a6a05703a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:12:59.269969Z",
     "start_time": "2025-04-23T23:12:59.140017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Needs Python 3.11 or older\n",
    "!python --version"
   ],
   "id": "dd23fc32d2edb5a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.16\r\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T23:12:59.757915Z",
     "start_time": "2025-04-23T23:12:59.274030Z"
    }
   },
   "source": "!pip install hazm",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hazm in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (0.10.0)\r\n",
      "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from hazm) (0.9.2)\r\n",
      "Requirement already satisfied: flashtext<3.0,>=2.7 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from hazm) (2.7)\r\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from hazm) (4.3.3)\r\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from hazm) (3.9.1)\r\n",
      "Requirement already satisfied: numpy==1.24.3 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from hazm) (1.24.3)\r\n",
      "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from hazm) (0.9.11)\r\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from hazm) (1.6.1)\r\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.13.6)\r\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (75.8.0)\r\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.13.1)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.1.0)\r\n",
      "Requirement already satisfied: click in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.8)\r\n",
      "Requirement already satisfied: joblib in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (2024.11.6)\r\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.67.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.6.0)\r\n",
      "Requirement already satisfied: wrapt in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.17.2)\r\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 💡 Part 2: Preprocessing\n",
    "\n",
    "In this section we’ll normalize raw Persian text and then split it into sentences and words using Hazm’s built-in tools."
   ],
   "id": "8b1c087c0850cb7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 📦 Imports",
   "id": "46948209ea3184c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:12:59.765030Z",
     "start_time": "2025-04-23T23:12:59.763409Z"
    }
   },
   "cell_type": "code",
   "source": "from hazm import Normalizer, sent_tokenize, word_tokenize",
   "id": "af3c6c6575658de4",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🔹 2.1 Text Normalization\n",
    "\n",
    "Hazm’s `Normalizer` handles common Persian‐specific cleanup (Unicode variants, diacritics, punctuation unification, etc.)."
   ],
   "id": "d969234aab46bdcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:00.326227Z",
     "start_time": "2025-04-23T23:12:59.768744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "normalizer = Normalizer()\n",
    "\n",
    "raw_text = 'سلام!  چطورید؟   این متن، شامل  چند  فاصلهٔ اضافه  و نیم‌فاصله‌است.'\n",
    "\n",
    "norm_text = normalizer.normalize(raw_text)\n",
    "print(norm_text)"
   ],
   "id": "6d696b84e999607f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام! چطورید؟ این متن، شامل چند فاصله اضافه و نیم‌فاصله‌است.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🔹 2.2 Sentence Tokenization\n",
    "\n",
    "Use `sent_tokenize` to split the normalized text into sentences."
   ],
   "id": "e010d70b1eb6ca76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:00.333096Z",
     "start_time": "2025-04-23T23:13:00.331505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences = sent_tokenize(norm_text)\n",
    "\n",
    "for sent in sentences:\n",
    "    print(sent)"
   ],
   "id": "99db3741aa9c0b56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام!\n",
      "چطورید؟\n",
      "این متن، شامل چند فاصله اضافه و نیم‌فاصله‌است.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🔹 2.3 Word Tokenization\n",
    "Use `word_tokenize` to split each sentence into word tokens."
   ],
   "id": "8beb0bbacd5d2d15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:00.348962Z",
     "start_time": "2025-04-23T23:13:00.347180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for sent in sentences:\n",
    "    tokens = word_tokenize(sent)\n",
    "    print(tokens)"
   ],
   "id": "6a58554cda6ced6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['سلام', '!']\n",
      "['چطورید', '؟']\n",
      "['این', 'متن', '،', 'شامل', 'چند', 'فاصله', 'اضافه', 'و', 'نیم\\u200cفاصله\\u200cاست', '.']\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ⭐ Tip — Customize the Normalizer\n",
    "\n",
    "Suppose you want to **keep Western digits** and **diacritics**. You can customize the `Normalizer` like this:"
   ],
   "id": "7ba591ac84f4ea50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:01.469241Z",
     "start_time": "2025-04-23T23:13:00.361905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from hazm import Normalizer\n",
    "\n",
    "raw_text = 'می‌روم؛ 123؟ این یِک مَتنِ آزمایشی‌ست.'\n",
    "\n",
    "default_norm = Normalizer()\n",
    "print(default_norm.normalize(raw_text))\n",
    "\n",
    "custom_norm = Normalizer(\n",
    "    remove_diacritics=False,\n",
    "    persian_numbers=False,\n",
    ")\n",
    "print(custom_norm.normalize(raw_text))"
   ],
   "id": "eadf5bf00677db7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "می‌روم؛ ۱۲۳؟ این یک متن آزمایشی‌ست.\n",
      "می‌روم؛ 123؟ این یِک مَتنِ آزمایشی‌ست.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧠 Part 3: Stemming & Lemmatization\n",
    "\n",
    "In this part, we’ll reduce words to their base forms using Hazm’s **Stemmer** and **Lemmatizer**. Stemmer is rule-based (heuristic), while Lemmatizer uses a dictionary for more accurate results."
   ],
   "id": "bb9f55530e42d87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 📦 Imports",
   "id": "6c79278811284629"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:01.475405Z",
     "start_time": "2025-04-23T23:13:01.473797Z"
    }
   },
   "cell_type": "code",
   "source": "from hazm import Stemmer, Lemmatizer, word_tokenize",
   "id": "7380ad747af7d276",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 🔹 3.1 Stemming",
   "id": "15ca7bc837b50f68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:01.480913Z",
     "start_time": "2025-04-23T23:13:01.479192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stemmer = Stemmer()\n",
    "\n",
    "words = ['می‌روم', 'رفتن', 'رفتند', 'دوستان', 'کتاب‌ها']\n",
    "\n",
    "stems = [stemmer.stem(w) for w in words]\n",
    "print(words)  # Original\n",
    "print(stems)  # Stemmed"
   ],
   "id": "48db300823f47915",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['می\\u200cروم', 'رفتن', 'رفتند', 'دوستان', 'کتاب\\u200cها']\n",
      "['می\\u200cرو', 'رفتن', 'رفتند', 'دوس', 'کتاب']\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 🔹 3.2 Lemmatization",
   "id": "aa3867d527b67f80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:01.843990Z",
     "start_time": "2025-04-23T23:13:01.488943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "words = ['می‌روم', 'رفتن', 'رفتند', 'دوستان', 'کتاب‌ها']\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in words]\n",
    "print(words)  # Original\n",
    "print(lemmas)  # Lemmatized"
   ],
   "id": "4b477d4da5c0b059",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['می\\u200cروم', 'رفتن', 'رفتند', 'دوستان', 'کتاب\\u200cها']\n",
      "['رفت#رو', 'رفتن', 'رفت#رو', 'دوستان', 'کتاب']\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 🔍 When to Use Which?\n",
    "\n",
    "- **Stemmer** is fast and language-agnostic but may over- or under-stem.\n",
    "- **Lemmatizer** is more accurate for Persian morphology but requires its dictionary."
   ],
   "id": "d9c2c6bc8a4a4a0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ✍️ Part 4: POS Tagging & Chunking",
   "id": "4d6f88d578bded3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 📦 Imports & Model Download\n",
    "\n",
    "Download the pretrained \"POSTagger\" model from [Hazm’s GitHub Pretrained-Models section](https://github.com/roshan-research/hazm?tab=readme-ov-file#pretrained-models) and save it in:<br>\n",
    "`resources/pos_tagger.model`\n",
    "\n",
    "Download the pretrained \"Chunker\" model from [Hazm’s GitHub Pretrained-Models section](https://github.com/roshan-research/hazm?tab=readme-ov-file#pretrained-models) and save it in:<br>\n",
    "`resources/chunker.model`"
   ],
   "id": "b27478ad422393f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:01.849993Z",
     "start_time": "2025-04-23T23:13:01.848321Z"
    }
   },
   "cell_type": "code",
   "source": "from hazm import POSTagger, Chunker, word_tokenize, tree2brackets",
   "id": "c9be00dbff9e57b5",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 🔹 4.1 POS Tagging",
   "id": "ee01d445b6791611"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:01.862577Z",
     "start_time": "2025-04-23T23:13:01.857343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tagger = POSTagger(model='resources/pos_tagger.model')\n",
    "\n",
    "sentence = 'من دوست دارم زبان فارسی را بهتر یاد بگیرم.'\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "tags = tagger.tag(tokens)\n",
    "print(tags)"
   ],
   "id": "938c2d433122ca03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('من', 'PRON'), ('دوست', 'NOUN'), ('دارم', 'VERB'), ('زبان', 'NOUN,EZ'), ('فارسی', 'NOUN'), ('را', 'ADP'), ('بهتر', 'ADJ'), ('یاد', 'NOUN'), ('بگیرم', 'VERB'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 🔹 4.2 Chunking",
   "id": "1996f85617bba48f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:01.873700Z",
     "start_time": "2025-04-23T23:13:01.871181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunker = Chunker(model='resources/chunker.model')\n",
    "chunk_tree = chunker.parse(tags)"
   ],
   "id": "ffb993420d336886",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:01.882655Z",
     "start_time": "2025-04-23T23:13:01.881146Z"
    }
   },
   "cell_type": "code",
   "source": "print(chunk_tree)",
   "id": "c3484583aa70c67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP من/PRON)\n",
      "  (VP دوست/NOUN دارم/VERB)\n",
      "  (NP زبان/NOUN,EZ فارسی/NOUN)\n",
      "  (POSTP را/ADP)\n",
      "  (ADJP بهتر/ADJ)\n",
      "  (VP یاد/NOUN بگیرم/VERB)\n",
      "  ./PUNCT)\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:01.896457Z",
     "start_time": "2025-04-23T23:13:01.894468Z"
    }
   },
   "cell_type": "code",
   "source": "tree2brackets(chunk_tree)",
   "id": "e708868dce0d3212",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[من NP] [دوست دارم VP] [زبان فارسی NP] [را POSTP] [بهتر ADJP] [یاد بگیرم VP] .'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> `NP` denotes noun phrases, `VP` verb phrases, giving you easy access to phrase-level units for further analysis.",
   "id": "ff0fc5ce1be700f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 🌳 Part 5: Dependency Parsing",
   "id": "50842087e55eb13e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 📦 Imports & Model Download\n",
    "\n",
    "Download the pretrained \"DependencyParser\" model from [Hazm’s GitHub Pretrained-Models section](https://github.com/roshan-research/hazm?tab=readme-ov-file#pretrained-models) and save it as:\n",
    "\n",
    "`resources/universal_dependency_parser`"
   ],
   "id": "90dd5a0ec190f35a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:01.910245Z",
     "start_time": "2025-04-23T23:13:01.908708Z"
    }
   },
   "cell_type": "code",
   "source": "from hazm import POSTagger, Lemmatizer, DependencyParser, word_tokenize",
   "id": "b9e474f6e576242e",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 🔹 5.1 Initialization",
   "id": "372eeb1aa0e53bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:02.261212Z",
     "start_time": "2025-04-23T23:13:01.917101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tagger = POSTagger(model='resources/pos_tagger.model')\n",
    "lemmatizer = Lemmatizer()\n",
    "parser = DependencyParser(tagger=tagger, lemmatizer=lemmatizer, working_dir='resources/universal_dependency_parser')"
   ],
   "id": "bfb4e177f133661b",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 🔹 5.2 Parse a Sentence",
   "id": "32b516a4e64f797c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:03.429361Z",
     "start_time": "2025-04-23T23:13:02.266471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence = 'من دوست دارم زبان فارسی را بهتر یاد بگیرم.'\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "graph = parser.parse(tokens)\n",
    "print(graph)"
   ],
   "id": "97a8646ef0e9f7af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x364cb3be0>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'root': [3], 'ROOT': []}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'PRON',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 3,\n",
      "                 'lemma': 'من',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'PRON',\n",
      "                 'word': 'من'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'NOUN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 3,\n",
      "                 'lemma': 'دوست',\n",
      "                 'rel': 'compound',\n",
      "                 'tag': 'NOUN',\n",
      "                 'word': 'دوست'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'VERB',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'ccomp': [9],\n",
      "                                      'compound': [2],\n",
      "                                      'nsubj': [1],\n",
      "                                      'punct': [10]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': 'دار',\n",
      "                 'rel': 'root',\n",
      "                 'tag': 'VERB',\n",
      "                 'word': 'دارم'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'NOUN,EZ',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'amod': [5],\n",
      "                                      'case': [6]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': 'زبان',\n",
      "                 'rel': 'obj',\n",
      "                 'tag': 'NOUN,EZ',\n",
      "                 'word': 'زبان'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'NOUN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': 'فارسی',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'NOUN',\n",
      "                 'word': 'فارسی'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'ADP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': 'را',\n",
      "                 'rel': 'case',\n",
      "                 'tag': 'ADP',\n",
      "                 'word': 'را'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'ADJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': 'بهتر',\n",
      "                 'rel': 'advmod',\n",
      "                 'tag': 'ADJ',\n",
      "                 'word': 'بهتر'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'NOUN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': 'یاد',\n",
      "                 'rel': 'compound',\n",
      "                 'tag': 'NOUN',\n",
      "                 'word': 'یاد'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'VERB',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'advmod': [7],\n",
      "                                      'compound': [8],\n",
      "                                      'obj': [4]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 3,\n",
      "                 'lemma': 'بگیر',\n",
      "                 'rel': 'ccomp',\n",
      "                 'tag': 'VERB',\n",
      "                 'word': 'بگیرم'},\n",
      "             10: {'address': 10,\n",
      "                  'ctag': 'PUNCT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 3,\n",
      "                  'lemma': '.',\n",
      "                  'rel': 'punct',\n",
      "                  'tag': 'PUNCT',\n",
      "                  'word': '.'}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages/nltk/parse/dependencygraph.py:376: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 📖 Quick Guide: What’s in `graph.nodes[idx]`?\n",
    "\n",
    "Each node is a Python dict with these keys:\n",
    "\n",
    "| Key       | Meaning                                                                                           |\n",
    "|-----------|---------------------------------------------------------------------------------------------------|\n",
    "| `address` | 1-based position of the token in the sentence (0 = artificial ROOT)                               |\n",
    "| `word`    | Surface form of the token                                                                         |\n",
    "| `lemma`   | Lemmatized form (from Hazm’s lemmatizer)                                                          |\n",
    "| `tag`     | Fine-grained POS tag (Bijankhan tagset) e.g. `N`, `V`, `ADJ`, `PREP`                              |\n",
    "| `ctag`    | Coarse POS tag (Universal POS) e.g. `NOUN`, `VERB`, `ADJ`, `ADP`, `PRON`                          |\n",
    "| `feats`   | Morphological features string (often `_` if unavailable) e.g. `Number=Sing\\|Person=1\\|Tense=Past` |\n",
    "| `head`    | Address of the syntactic head (0 if this token is the root)                                       |\n",
    "| `rel`     | Dependency relation to the head (e.g., `nsubj`, `obj`, `advmod`)                                  |\n",
    "| `deps`    | Dict mapping each child-relation label → list of child addresses                                  |"
   ],
   "id": "449d66070487e3bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:03.437422Z",
     "start_time": "2025-04-23T23:13:03.435081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "node = graph.nodes[1]\n",
    "node"
   ],
   "id": "112c1212323c28b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': 1,\n",
       " 'word': 'من',\n",
       " 'lemma': 'من',\n",
       " 'ctag': 'PRON',\n",
       " 'tag': 'PRON',\n",
       " 'feats': '_',\n",
       " 'head': 3,\n",
       " 'deps': defaultdict(list, {}),\n",
       " 'rel': 'nsubj'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 🔚 Part 6: Summary",
   "id": "8c6c778e84643589"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ✅ What We’ve Covered in Hazm\n",
    "\n",
    "| Component                 | Hazm API                                                         | Notes                                                |\n",
    "|---------------------------|------------------------------------------------------------------|------------------------------------------------------|\n",
    "| **Normalization**         | `Normalizer().normalize(text)`                                   | Unicode variants, diacritics, spacing, numbers       |\n",
    "| **Sentence Tokenization** | `sent_tokenize(text)`                                            | Persian‐specific rules                               |\n",
    "| **Word Tokenization**     | `word_tokenize(sentence)`                                        | Handles Persian clitics and punctuation              |\n",
    "| **Stemming**              | `Stemmer().stem(word)`                                           | Fast, rule‐based                                     |\n",
    "| **Lemmatization**         | `Lemmatizer().lemmatize(word)`                                   | Dictionary‐based, more accurate                      |\n",
    "| **POS Tagging**           | `POSTagger(model='resources/pos_tagger.model')` → `.tag(tokens)` | Bijankhan tagset                                     |\n",
    "| **Chunking**              | `Chunker(model='resources/chunker.model').parse(tags)`           | Parses POS tags into phrases                         |\n",
    "| **Dependency Parsing**    | `DependencyParser(...).parse(tokens)`                            | MaltParser under the hood, returns `DependencyGraph` |"
   ],
   "id": "67d69d38c2c1769a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧪 Part 7: Integrated Pipeline Exercise\n",
    "\n",
    "Instead of running each NLP step separately, let’s build one **`preprocess_hazm`** function that:\n",
    "\n",
    "1. **Normalizes** raw text\n",
    "2. **Sentence-tokenizes** & **word-tokenizes**\n",
    "3. **Removes punctuation**\n",
    "4. **POS-filters** (keep only nouns, verbs, adjectives, adverbs)\n",
    "5. **Stems** or **lemmatizes** (configurable)\n",
    "6. **Lowercases** the final tokens\n",
    "\n",
    "Then we’ll wrap it in a simple class and compare performance using stemming vs. lemmatization.\n"
   ],
   "id": "1c750e6f34c7bf99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🔹 7.1 Write `preprocess_hazm`",
   "id": "2ba1ed01fdd8080e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:04.399726Z",
     "start_time": "2025-04-23T23:13:03.453217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from hazm import Normalizer, sent_tokenize, word_tokenize, POSTagger, Stemmer, Lemmatizer\n",
    "import string\n",
    "\n",
    "# 1. Instantiate components\n",
    "normalizer = Normalizer()\n",
    "postagger = POSTagger(model='resources/pos_tagger.model')\n",
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "# 2. Define punctuation set (including Persian marks)\n",
    "PUNCT = set(string.punctuation + '؟،؛…«»')\n",
    "\n",
    "\n",
    "def preprocess_hazm(text: str, use_lemmatizer: bool = True, allowed_pos: list[str] = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    - Normalize text\n",
    "    - Tokenize into sentences and words\n",
    "    - Remove punctuation tokens\n",
    "    - If allowed_pos is provided, filter tokens by those POS tags\n",
    "    - Otherwise keep all tokens\n",
    "    - Lemmatize or stem\n",
    "    - Lowercase\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ],
   "id": "a6bb4de26c673c6f",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:04.406213Z",
     "start_time": "2025-04-23T23:13:04.404367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = 'سلام! من عاشق پردازش زبان طبیعی با هضم هستم.'\n",
    "print(preprocess_hazm(sample, use_lemmatizer=True))\n",
    "print(preprocess_hazm(sample, use_lemmatizer=False))\n",
    "# Expected:\n",
    "    # ['سلام', 'من', 'عاشق', 'پردازش', 'زبان', 'طبیعی', 'با', 'هضم', '#هست']\n",
    "    # ['سلا', 'من', 'عاشق', 'پرداز', 'زب', 'طبیع', 'با', 'هض', 'هس']"
   ],
   "id": "9e746f55332b1c72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ✅ Answer",
   "id": "e15ced5742798668"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:05.322557Z",
     "start_time": "2025-04-23T23:13:04.419068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from hazm import Normalizer, sent_tokenize, word_tokenize, POSTagger, Stemmer, Lemmatizer\n",
    "import string\n",
    "\n",
    "# 1. Instantiate components\n",
    "normalizer = Normalizer()\n",
    "postagger = POSTagger(model='resources/pos_tagger.model')\n",
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "# 2. Define punctuation set (including Persian marks)\n",
    "PUNCT = set(string.punctuation + '؟،؛…«»')\n",
    "\n",
    "\n",
    "def preprocess_hazm(text: str, use_lemmatizer: bool = True, allowed_pos: list[str] = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    - Normalize text\n",
    "    - Tokenize into sentences and words\n",
    "    - Remove punctuation tokens\n",
    "    - If allowed_pos is provided, filter tokens by those POS tags\n",
    "    - Otherwise keep all tokens\n",
    "    - Lemmatize or stem\n",
    "    - Lowercase\n",
    "    \"\"\"\n",
    "    # Normalize\n",
    "    normed = normalizer.normalize(text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = []\n",
    "    for sent in sent_tokenize(normed):\n",
    "        for tok in word_tokenize(sent):\n",
    "            if tok in PUNCT:\n",
    "                continue\n",
    "            tokens.append(tok)\n",
    "\n",
    "    # POS tagging\n",
    "    tagged = postagger.tag(tokens)\n",
    "\n",
    "    # Filter by POS if requested\n",
    "    if allowed_pos is not None:\n",
    "        tagged = [(w, tag) for w, tag in tagged if tag in allowed_pos]\n",
    "\n",
    "    # Extract words\n",
    "    words = [w for w, _ in tagged]\n",
    "\n",
    "    # Normalize words\n",
    "    if use_lemmatizer:\n",
    "        processed = [lemmatizer.lemmatize(w) for w in words]\n",
    "    else:\n",
    "        processed = [stemmer.stem(w) for w in words]\n",
    "\n",
    "    # Lowercase\n",
    "    return [w.lower() for w in processed]"
   ],
   "id": "c6c35891dd382584",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:05.330131Z",
     "start_time": "2025-04-23T23:13:05.327566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = 'سلام! من عاشق پردازش زبان طبیعی با هضم هستم.'\n",
    "print(preprocess_hazm(sample, use_lemmatizer=True))\n",
    "print(preprocess_hazm(sample, use_lemmatizer=False))"
   ],
   "id": "eefb8c97eefb11c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['سلام', 'من', 'عاشق', 'پردازش', 'زبان', 'طبیعی', 'با', 'هضم', '#هست']\n",
      "['سلا', 'من', 'عاشق', 'پرداز', 'زب', 'طبیع', 'با', 'هض', 'هس']\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 🔹 7.2 Wrap as a “Pipeline” Class",
   "id": "9c28b7d1b267d346"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:05.341357Z",
     "start_time": "2025-04-23T23:13:05.339205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HazmPreprocessor:\n",
    "    def __init__(self, use_lemmatizer: bool = True, allowed_pos: list[str] = None):\n",
    "        self.use_lem = use_lemmatizer\n",
    "        self.allowed_pos = allowed_pos\n",
    "\n",
    "    def __call__(self, text: str) -> list[str]:\n",
    "        return preprocess_hazm(text, use_lemmatizer=self.use_lem, allowed_pos=self.allowed_pos)"
   ],
   "id": "5d58d631d858ee2e",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:05.347662Z",
     "start_time": "2025-04-23T23:13:05.345685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pp = HazmPreprocessor(use_lemmatizer=True)\n",
    "print(pp(sample))"
   ],
   "id": "6126461909a385d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['سلام', 'من', 'عاشق', 'پردازش', 'زبان', 'طبیعی', 'با', 'هضم', '#هست']\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🔹 7.3 Performance Comparison\n",
    "Measure throughput for a batch of sentences with vs. without lemmatization:"
   ],
   "id": "7a54f8cb92c41b97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:05.472099Z",
     "start_time": "2025-04-23T23:13:05.353221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "docs = [sample] * 500  # repeat sample 500×\n",
    "\n",
    "\n",
    "def time_it(preprocessor, docs):\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    for d in docs:\n",
    "        preprocessor(d)\n",
    "\n",
    "    return time.perf_counter() - start\n",
    "\n",
    "\n",
    "pp_lem = HazmPreprocessor(use_lemmatizer=True)\n",
    "pp_stem = HazmPreprocessor(use_lemmatizer=False)\n",
    "\n",
    "t_lem = time_it(pp_lem, docs)\n",
    "t_stem = time_it(pp_stem, docs)\n",
    "\n",
    "print(f'Lemmatizer time: {t_lem:.3f}s')\n",
    "print(f'Stemmer time:    {t_stem:.3f}s')"
   ],
   "id": "a67d7bb3b7e6cf44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizer time: 0.058s\n",
      "Stemmer time:    0.058s\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:13:05.482138Z",
     "start_time": "2025-04-23T23:13:05.480836Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5f70f854a05f900e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
