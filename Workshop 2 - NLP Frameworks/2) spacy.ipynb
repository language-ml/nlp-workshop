{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src='https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png' width=150 height=150> <br>\n",
    "<font color=0F5298 size=6>\n",
    "Natural Language Processing<br>\n",
    "<font color=2565AE size=4>\n",
    "Computer Engineering Department<br>\n",
    "Spring 2025<br>\n",
    "<font color=3C99D size=4>\n",
    "Workshop 1 - NLP Frameworks - spaCy<br>\n",
    "<font color=696880 size=3>\n",
    "<a href='https://language.ml'>https://language.ml</a><br>\n",
    "info [AT] language [dot] ml"
   ],
   "id": "9e01f7699443e653"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìñ Part 1: Introduction\n",
    "\n",
    "## ‚ùì What is spaCy? ([spaCy Official Website](https://spacy.io))\n",
    "\n",
    "**spaCy** is a modern, fast, and industrial-strength NLP library written in Python and Cython. It provides high-performance pipelines and pretrained models for many key NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ When to Use spaCy\n",
    "\n",
    "- **Production Applications:** Optimized for speed and scalability.\n",
    "- **Pretrained Models:** Ready-to-use, high-accuracy models for tokenization, tagging, parsing, NER, and more.\n",
    "- **Deep Learning Integration:** Seamlessly integrates with transformer libraries (e.g. via `spacy-transformers`).\n",
    "- **Modular Pipelines:** Customize and extend via simple APIs.\n",
    "\n",
    "---\n",
    "\n",
    "## üö´ spaCy might not be ideal for:\n",
    "\n",
    "- **Teaching Classical NLP:** NLTK is better for learning symbolic NLP tasks like treebanks or CFG parsing.\n",
    "- **Highly Custom Tokenization or Parsing Logic:** spaCy offers custom rules, but for very experimental or rule-heavy systems, NLTK may still be better.\n",
    "- **Resource-Constrained Environments:** Its models can be heavier than NLTK's rule-based ones."
   ],
   "id": "83a2820d668230f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ‚öôÔ∏è Installation & Setup",
   "id": "a2ae6c0a47ca9d8b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:56.892731Z",
     "start_time": "2025-04-23T18:31:32.136364Z"
    }
   },
   "source": [
    "# Install spaCy and the small English model\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_md"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (3.8.5)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (1.0.12)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (2.0.11)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (8.3.6)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (1.1.3)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (2.5.1)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (0.4.1)\r\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (0.15.2)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (4.67.1)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (2.2.5)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (2.32.3)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (2.11.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (3.1.6)\r\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (75.8.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (25.0)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from spacy) (3.5.0)\r\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\r\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\r\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\r\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\r\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\r\n",
      "Requirement already satisfied: wrapt in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\r\n",
      "Collecting en-core-web-sm==3.8.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\r\n",
      "\u001B[2K     \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h\u001B[38;5;2m‚úî Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n",
      "Collecting en-core-web-md==3.8.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\r\n",
      "\u001B[2K     \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m33.5/33.5 MB\u001B[0m \u001B[31m2.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h\u001B[38;5;2m‚úî Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_md')\r\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:56.897768Z",
     "start_time": "2025-04-23T18:31:56.895692Z"
    }
   },
   "cell_type": "code",
   "source": "import spacy",
   "id": "fbedf67cd199cfcc",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.416599Z",
     "start_time": "2025-04-23T18:31:56.903307Z"
    }
   },
   "cell_type": "code",
   "source": "spacy.cli.download('en_core_web_md')",
   "id": "c96888acab95105c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001B[38;5;2m‚úî Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "\u001B[38;5;3m‚ö† Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üí° Part 2: Primer ‚Äî What are `nlp` and `doc`\n",
    "\n",
    "Before we explore spaCy‚Äôs features, let‚Äôs clarify two core concepts:"
   ],
   "id": "dee8551126671a84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîπ `nlp`: The Processing Pipeline\n",
    "\n",
    "- Loads a **pretrained English model** with tokenization, tagging, parsing, NER, etc.\n",
    "- `nlp` is a **callable pipeline**: pass text in, get a processed `Doc`."
   ],
   "id": "28230f5cf83ecec1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.575757Z",
     "start_time": "2025-04-23T18:31:59.422325Z"
    }
   },
   "cell_type": "code",
   "source": "nlp = spacy.load('en_core_web_sm')",
   "id": "57abf64ab61d636a",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîπ `doc`: The Processed Text\n",
    "\n",
    "- A `doc` is a `spacy.tokens.Doc` object, representing the full processed text.\n",
    "- It contains:\n",
    "    - Tokens: `doc[i]`\n",
    "    - Sentences: `list(doc.sents)`\n",
    "    - Entities: `doc.ents`\n",
    "    - Annotations: POS tags, lemmas, dependencies, etc."
   ],
   "id": "c7722d551751d78a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.585050Z",
     "start_time": "2025-04-23T18:31:59.580231Z"
    }
   },
   "cell_type": "code",
   "source": "doc = nlp(\"Apple is looking at buying a startup in the UK.\")",
   "id": "364d875e845b421c",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üîç Quick Check",
   "id": "24ec67f468d0d272"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.590808Z",
     "start_time": "2025-04-23T18:31:59.588877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Tokens\n",
    "print('Example token [doc[0]]:', doc[0], '‚Üí', type(doc[0]))\n",
    "\n",
    "# 2. Sentences\n",
    "print('Sentences [list(doc.sents)]:', list(doc.sents))\n",
    "\n",
    "# 3. Entities\n",
    "print('Entities [doc.ents]:', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "# 4. Annotations (POS, Lemma, Dependency)\n",
    "print('\\nAnnotations:')\n",
    "for token in doc:\n",
    "    print(f'{token.text:12} POS: {token.pos_:6}  Lemma: {token.lemma_:10}  Dep: {token.dep_:6}')"
   ],
   "id": "bd46d931fac54d1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example token [doc[0]]: Apple ‚Üí <class 'spacy.tokens.token.Token'>\n",
      "Sentences [list(doc.sents)]: [Apple is looking at buying a startup in the UK.]\n",
      "Entities [doc.ents]: [('Apple', 'ORG'), ('UK', 'GPE')]\n",
      "\n",
      "Annotations:\n",
      "Apple        POS: PROPN   Lemma: Apple       Dep: nsubj \n",
      "is           POS: AUX     Lemma: be          Dep: aux   \n",
      "looking      POS: VERB    Lemma: look        Dep: ROOT  \n",
      "at           POS: ADP     Lemma: at          Dep: prep  \n",
      "buying       POS: VERB    Lemma: buy         Dep: pcomp \n",
      "a            POS: DET     Lemma: a           Dep: det   \n",
      "startup      POS: NOUN    Lemma: startup     Dep: dobj  \n",
      "in           POS: ADP     Lemma: in          Dep: prep  \n",
      "the          POS: DET     Lemma: the         Dep: det   \n",
      "UK           POS: PROPN   Lemma: UK          Dep: pobj  \n",
      ".            POS: PUNCT   Lemma: .           Dep: punct \n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.604381Z",
     "start_time": "2025-04-23T18:31:59.602236Z"
    }
   },
   "cell_type": "code",
   "source": "spacy.explain('GPE'), spacy.explain('PROPN'), spacy.explain('nsubj'), spacy.explain('aux')",
   "id": "313f27abfa6b44d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Countries, cities, states', 'proper noun', 'nominal subject', 'auxiliary')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üß† Part 3: Core Linguistic Features",
   "id": "dc967c6caf76123"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üîπ 3.1 Tokenization & Sentence Segmentation",
   "id": "c79d5403e2a3f299"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.625640Z",
     "start_time": "2025-04-23T18:31:59.620007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"SpaCy is a powerful NLP library. It‚Äôs designed for production use.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Sentences\n",
    "print('Sentences:', [sent.text for sent in doc.sents])\n",
    "\n",
    "# Word tokens\n",
    "print('Tokens:', [token.text for token in doc])"
   ],
   "id": "e91e5ae67ac553b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['SpaCy is a powerful NLP library.', 'It‚Äôs designed for production use.']\n",
      "Tokens: ['SpaCy', 'is', 'a', 'powerful', 'NLP', 'library', '.', 'It', '‚Äôs', 'designed', 'for', 'production', 'use', '.']\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üîπ 3.2 POS Tagging & Morphology",
   "id": "6d6543bf4911dc26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üÜö `token.pos_` vs `token.tag_` ‚Äî What's the Difference?\n",
    "\n",
    "spaCy provides **two levels of part-of-speech (POS) tagging** for each token:\n",
    "\n",
    "#### üîπ `token.pos_`: Universal POS Tag\n",
    "- This gives a **coarse-grained, language-independent** part of speech.\n",
    "- Comes from the **Universal POS tag set** (used across many languages).\n",
    "- Examples:\n",
    "  - `'NOUN'`, `'VERB'`, `'ADJ'`, `'ADV'`, `'PROPN'`, `'AUX'`, `'DET'`, `'ADP'`\n",
    "\n",
    "üìå Use this when you want a **high-level, consistent POS tag** (e.g., when training multilingual models).\n",
    "\n",
    "#### üîπ `token.tag_`: Language-Specific Detailed Tag\n",
    "- This gives a **fine-grained, language-specific** POS tag.\n",
    "- In English models, it uses the **Penn Treebank tag set**.\n",
    "- Examples:\n",
    "  - `'NN'` (singular noun), `'NNS'` (plural noun), `'VBZ'` (3rd person singular verb), `'VBN'` (past participle), `'JJ'` (adjective)\n",
    "\n",
    "üìå Use this when you need **grammatical detail** (e.g., distinguishing singular vs. plural, tense, voice)."
   ],
   "id": "d96ff3408b6c9533"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.639877Z",
     "start_time": "2025-04-23T18:31:59.635785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = \"She walks.\"\n",
    "sample_doc = nlp(sample)\n",
    "sample_token = sample_doc[1]  # \"walks\"\n",
    "\n",
    "print('sample_token.text:', sample_token.text)\n",
    "print('sample_token.pos_:', sample_token.pos_)\n",
    "print('sample_token.tag_:', sample_token.tag_)"
   ],
   "id": "5338b50f2717d662",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_token.text: walks\n",
      "sample_token.pos_: VERB\n",
      "sample_token.tag_: VBZ\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üìñ Understanding Morphological Features\n",
    "\n",
    "In spaCy, the `token.morph` attribute provides **morphological information** about each token‚Äîi.e., the grammatical features that describe how a word is inflected. These features include:\n",
    "\n",
    "- **Number** (e.g., `Number=Sing` for singular, `Number=Plur` for plural)\n",
    "- **Tense** (e.g., `Tense=Pres` for present, `Tense=Past` for past)\n",
    "- **Mood** (e.g., `Mood=Ind` for indicative, `Mood=Imp` for imperative)\n",
    "- **Person** (e.g., `Person=3` for third person)\n",
    "- **Aspect** (e.g., `Aspect=Perf` for perfective)\n",
    "- **Degree** (for adjectives/adverbs, e.g., `Degree=Pos` for positive, `Degree=Cmp` for comparative)\n",
    "- **Definiteness** and **Pronoun Types** (for determiners/pronouns)\n",
    "- **Verb Form** (e.g., `VerbForm=Fin` for finite forms, `VerbForm=Part` for participles)\n",
    "- **PunctType** (for punctuation, e.g., `PunctType=Peri` for period)\n",
    "\n",
    "These features are returned as a pipe-separated string."
   ],
   "id": "235067e33631b581"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.651485Z",
     "start_time": "2025-04-23T18:31:59.649783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text:12} POS: {token.pos_:8} Tag: {token.tag_:6} Morph: {token.morph}')"
   ],
   "id": "1e80993e3173899",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy        POS: PROPN    Tag: NNP    Morph: Number=Sing\n",
      "is           POS: AUX      Tag: VBZ    Morph: Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "a            POS: DET      Tag: DT     Morph: Definite=Ind|PronType=Art\n",
      "powerful     POS: ADJ      Tag: JJ     Morph: Degree=Pos\n",
      "NLP          POS: PROPN    Tag: NNP    Morph: Number=Sing\n",
      "library      POS: NOUN     Tag: NN     Morph: Number=Sing\n",
      ".            POS: PUNCT    Tag: .      Morph: PunctType=Peri\n",
      "It           POS: PRON     Tag: PRP    Morph: Gender=Neut|Number=Sing|Person=3|PronType=Prs\n",
      "‚Äôs           POS: AUX      Tag: VBD    Morph: Tense=Past|VerbForm=Fin\n",
      "designed     POS: VERB     Tag: VBN    Morph: Aspect=Perf|Tense=Past|VerbForm=Part\n",
      "for          POS: ADP      Tag: IN     Morph: \n",
      "production   POS: NOUN     Tag: NN     Morph: Number=Sing\n",
      "use          POS: NOUN     Tag: NN     Morph: Number=Sing\n",
      ".            POS: PUNCT    Tag: .      Morph: PunctType=Peri\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.666022Z",
     "start_time": "2025-04-23T18:31:59.663831Z"
    }
   },
   "cell_type": "code",
   "source": "spacy.explain('AUX'), spacy.explain('VBZ')",
   "id": "8deff3b79e3c553b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('auxiliary', 'verb, 3rd person singular present')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üîπ 3.3 Lemmatization",
   "id": "c2e6c8722b377394"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.680352Z",
     "start_time": "2025-04-23T18:31:59.678767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text:12} Lemma: {token.lemma_}')"
   ],
   "id": "eaf236dc26b42c3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy        Lemma: SpaCy\n",
      "is           Lemma: be\n",
      "a            Lemma: a\n",
      "powerful     Lemma: powerful\n",
      "NLP          Lemma: NLP\n",
      "library      Lemma: library\n",
      ".            Lemma: .\n",
      "It           Lemma: it\n",
      "‚Äôs           Lemma: ‚Äôs\n",
      "designed     Lemma: design\n",
      "for          Lemma: for\n",
      "production   Lemma: production\n",
      "use          Lemma: use\n",
      ".            Lemma: .\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üîπ 3.4 Dependency Parsing",
   "id": "dfaa4d3b0e053bae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.695751Z",
     "start_time": "2025-04-23T18:31:59.692543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# Visualize dependency tree in the notebook\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ],
   "id": "282fe16051e93625",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6723998c570f400285dd33426bb5fe0f-0\" class=\"displacy\" width=\"2150\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">SpaCy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">powerful</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">NLP</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">library.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">It</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">‚Äôs</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">designed</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">production</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">use.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6723998c570f400285dd33426bb5fe0f-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6723998c570f400285dd33426bb5fe0f-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6723998c570f400285dd33426bb5fe0f-0-1\" stroke-width=\"2px\" d=\"M420,352.0 C420,89.5 920.0,89.5 920.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6723998c570f400285dd33426bb5fe0f-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6723998c570f400285dd33426bb5fe0f-0-2\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6723998c570f400285dd33426bb5fe0f-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6723998c570f400285dd33426bb5fe0f-0-3\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6723998c570f400285dd33426bb5fe0f-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6723998c570f400285dd33426bb5fe0f-0-4\" stroke-width=\"2px\" d=\"M245,352.0 C245,2.0 925.0,2.0 925.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6723998c570f400285dd33426bb5fe0f-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,354.0 L933.0,342.0 917.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6723998c570f400285dd33426bb5fe0f-0-5\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,177.0 1440.0,177.0 1440.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6723998c570f400285dd33426bb5fe0f-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6723998c570f400285dd33426bb5fe0f-0-6\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6723998c570f400285dd33426bb5fe0f-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6723998c570f400285dd33426bb5fe0f-0-7\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,264.5 1610.0,264.5 1610.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6723998c570f400285dd33426bb5fe0f-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1610.0,354.0 L1618.0,342.0 1602.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6723998c570f400285dd33426bb5fe0f-0-8\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,264.5 1960.0,264.5 1960.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6723998c570f400285dd33426bb5fe0f-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,354.0 L1812,342.0 1828,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6723998c570f400285dd33426bb5fe0f-0-9\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,177.0 1965.0,177.0 1965.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6723998c570f400285dd33426bb5fe0f-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1965.0,354.0 L1973.0,342.0 1957.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.712109Z",
     "start_time": "2025-04-23T18:31:59.710377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text:12} Head: {token.head.text:12} Dep: {token.dep_}')"
   ],
   "id": "9f47ccfa8422bcda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy        Head: is           Dep: nsubj\n",
      "is           Head: is           Dep: ROOT\n",
      "a            Head: library      Dep: det\n",
      "powerful     Head: library      Dep: amod\n",
      "NLP          Head: library      Dep: compound\n",
      "library      Head: is           Dep: attr\n",
      ".            Head: is           Dep: punct\n",
      "It           Head: designed     Dep: nsubjpass\n",
      "‚Äôs           Head: designed     Dep: auxpass\n",
      "designed     Head: designed     Dep: ROOT\n",
      "for          Head: designed     Dep: prep\n",
      "production   Head: use          Dep: compound\n",
      "use          Head: for          Dep: pobj\n",
      ".            Head: designed     Dep: punct\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üîπ 3.5 Named Entity Recognition (NER)",
   "id": "9b6f5c00a3a9e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.725973Z",
     "start_time": "2025-04-23T18:31:59.724224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.text:20} Label: {ent.label_}')"
   ],
   "id": "3fcf0218ee46ac7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy                Label: PERSON\n",
      "NLP                  Label: ORG\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.740834Z",
     "start_time": "2025-04-23T18:31:59.738572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# style (str): Visualisation style, 'dep' or 'ent'.\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ],
   "id": "f519d8b096823eb3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SpaCy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is a powerful \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NLP\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " library. It‚Äôs designed for production use.</div></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîπ 3.6 Sentence Segmentation\n",
    "\n",
    "By default, spaCy splits text into sentences based on the model‚Äôs built-in rules and punctuation"
   ],
   "id": "41a3de7e41f696e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.762439Z",
     "start_time": "2025-04-23T18:31:59.753984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"SpaCy is amazing. It can detect sentence boundaries accurately‚Äîeven with abbreviations like U.S.A.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print each sentence separately\n",
    "for i, sent in enumerate(doc.sents, 1):\n",
    "    print(f\"Sentence {i}:\", sent.text)"
   ],
   "id": "410b88a4c2acb893",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: SpaCy is amazing.\n",
      "Sentence 2: It can detect sentence boundaries accurately‚Äîeven with abbreviations like U.S.A.\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîπ 3.7 Vectors & Similarity\n",
    "> Requires a model with word vectors, e.g. `en_core_web_md` or `en_core_web_lg`."
   ],
   "id": "e1e9d5e7911fc0e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:31:59.773062Z",
     "start_time": "2025-04-23T18:31:59.766738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc1 = nlp(\"NLP\")\n",
    "doc2 = nlp(\"natural language processing\")\n",
    "\n",
    "# Compute cosine similarity between the two Doc vectors\n",
    "print(f\"Similarity: {doc1.similarity(doc2):.2f}\")"
   ],
   "id": "39ad8e844622d551",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/kx7c6x1j16gdg23gt5g8mr440000gn/T/ipykernel_36302/1401424415.py:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(f\"Similarity: {doc1.similarity(doc2):.2f}\")\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.192739Z",
     "start_time": "2025-04-23T18:31:59.777317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp_md = spacy.load('en_core_web_md')\n",
    "\n",
    "doc1 = nlp_md(\"NLP\")\n",
    "doc2 = nlp_md(\"natural language processing\")\n",
    "\n",
    "# Compute cosine similarity between the two Doc vectors\n",
    "print(f\"Similarity: {doc1.similarity(doc2):.2f}\")"
   ],
   "id": "865030087f598888",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.09\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ü§ñ Part 4: Rule-based Matching\n",
    "\n",
    "spaCy allows you to find words, patterns, and structures in text using rule-based matchers. These are very fast and useful when:\n",
    "- You don‚Äôt need machine learning\n",
    "- You want precise, custom control over matching behavior"
   ],
   "id": "8d1b3c173baea167"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.346307Z",
     "start_time": "2025-04-23T18:32:00.197073Z"
    }
   },
   "cell_type": "code",
   "source": "nlp = spacy.load('en_core_web_sm')",
   "id": "9b19451dc59ddbba",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîπ 4.1 What is `nlp.vocab`?\n",
    "\n",
    "When you load a model with `nlp = spacy.load('en_core_web_sm')`, spaCy creates a shared vocabulary object:"
   ],
   "id": "de02bb5ce15af165"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.352297Z",
     "start_time": "2025-04-23T18:32:00.350438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = nlp.vocab\n",
    "vocab"
   ],
   "id": "b17e25344ec988fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.vocab.Vocab at 0x1496d5f30>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This object, of type `spacy.vocab.Vocab`, contains all language-specific data used across the NLP pipeline. It is:\n",
    "\n",
    "- **Shared by all components** (tokenizer, parser, tagger, matcher, etc.)\n",
    "- **Memory-efficient:** words are mapped to IDs instead of being repeated as strings"
   ],
   "id": "bc85988993da2def"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üì¶ `nlp.vocab` includes:\n",
    "\n",
    "- **Lexemes** ‚Äì Basic word representations (with spelling, shape, etc.)\n",
    "- **StringStore** ‚Äì Maps strings to IDs and vice versa\n",
    "- **Word Vectors** ‚Äì If available, dense embeddings for similarity\n",
    "- **Lookups** ‚Äì Custom mappings (e.g., for lemmatization)"
   ],
   "id": "484b3d516723b262"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.365531Z",
     "start_time": "2025-04-23T18:32:00.362844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lex = nlp_md.vocab['apple']\n",
    "\n",
    "# lex.is_alpha -> Is this word made of alphabetic characters only?\n",
    "# lex.shape_ -> The shape of the word (e.g., \"Xxxx\" for \"Apple\")\n",
    "print(lex.text, lex.is_alpha, lex.shape_)\n",
    "print(lex.vector)\n",
    "\n",
    "print(nlp.vocab.strings['dog'])"
   ],
   "id": "909e14ece58e8b72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple True xxxx\n",
      "[-0.6334     0.18981   -0.53544   -0.52658   -0.30001    0.30559\n",
      " -0.49303    0.14636    0.012273   0.96802    0.0040354  0.25234\n",
      " -0.29864   -0.014646  -0.24905   -0.67125   -0.053366   0.59426\n",
      " -0.068034   0.10315    0.66759    0.024617  -0.37548    0.52557\n",
      "  0.054449  -0.36748   -0.28013    0.090898  -0.025687  -0.5947\n",
      " -0.24269    0.28603    0.686      0.29737    0.30422    0.69032\n",
      "  0.042784   0.023701  -0.57165    0.70581   -0.20813   -0.03204\n",
      " -0.12494   -0.42933    0.31271    0.30352    0.09421   -0.15493\n",
      "  0.071356   0.15022   -0.41792    0.066394  -0.034546  -0.45772\n",
      "  0.57177   -0.82755   -0.27885    0.71801   -0.12425    0.18551\n",
      "  0.41342   -0.53997    0.55864   -0.015805  -0.1074    -0.29981\n",
      " -0.17271    0.27066    0.043996   0.60107   -0.353      0.6831\n",
      "  0.20703    0.12068    0.24852   -0.15605    0.25812    0.007004\n",
      " -0.10741   -0.097053   0.085628   0.096307   0.20857   -0.23338\n",
      " -0.077905  -0.030906   1.0494     0.55368   -0.10703    0.052234\n",
      "  0.43407   -0.13926    0.38115    0.021104  -0.40922    0.35972\n",
      " -0.28898    0.30618    0.060807  -0.023517   0.58193   -0.3098\n",
      "  0.21013   -0.15557   -0.56913   -1.1364     0.36598   -0.032666\n",
      "  1.1926     0.12825   -0.090486  -0.47965   -0.61164   -0.16484\n",
      " -0.41134    0.19925    0.059183  -0.20842    0.45223    0.27697\n",
      " -0.20745    0.025404  -0.28874    0.040478  -0.22275   -0.43323\n",
      "  0.76957   -0.054327  -0.35213   -0.30842   -0.48791   -0.35564\n",
      "  0.19813   -0.094767  -0.50918    0.18763   -0.087555   0.37709\n",
      " -0.1322    -0.096913  -1.9102     0.55813    0.27391   -0.077744\n",
      " -0.43933   -0.10367   -0.24408    0.41869    0.11659    0.27454\n",
      "  0.81021   -0.11006    0.43131    0.29095   -0.49548   -0.31958\n",
      " -0.072506   0.020286   0.2179     0.22032   -0.29212    0.75639\n",
      "  0.13598    0.019736  -0.83104    0.22836   -0.28669   -1.0529\n",
      "  0.052771   0.41266    0.50149    0.5323     0.51573   -0.31806\n",
      " -0.4619     0.21739   -0.43584   -0.41382    0.042237  -0.57179\n",
      "  0.067623  -0.27854    0.090044   0.20633    0.024678  -0.57703\n",
      " -0.020183  -0.53147   -0.37548   -0.12795   -0.093662  -0.0061183\n",
      "  0.20221   -0.62296   -0.29746    0.26935    0.59009   -0.50382\n",
      " -0.69757    0.20157   -0.33592   -0.45766    0.14061    0.22982\n",
      "  0.044046   0.26386    0.02942    0.34095    1.1496    -0.15555\n",
      " -0.064071   0.30139    0.024211  -0.63515   -0.73347   -0.10346\n",
      " -0.22637   -0.056392  -0.16735   -0.097331  -0.19206   -0.18866\n",
      "  0.15116   -0.038048   0.70205    0.11586   -0.14813    0.0095166\n",
      " -0.33804   -0.10158   -0.23829   -0.22759    0.092504  -0.29839\n",
      " -0.39721    0.26092    0.34594   -0.47396   -0.25725   -0.19257\n",
      " -0.53071    0.1692    -0.47252   -0.17333   -0.40505    0.046446\n",
      " -0.04473    0.33555   -0.5693     0.31591   -0.21167   -0.31298\n",
      " -0.45923   -0.083091   0.086822   0.01264    0.43779    0.12651\n",
      "  0.30156    0.022061   0.26549   -0.29455   -0.14838    0.033692\n",
      " -0.37346   -0.075343  -0.56498   -0.24207   -0.69351   -0.20277\n",
      " -0.0081185  0.030971   0.53615   -0.16613   -0.84087    0.74661\n",
      "  0.029132   0.46936   -0.49755    0.40954   -0.022558   0.21497\n",
      " -0.049528  -0.039799   0.46165    0.26456    0.32985   -0.04219\n",
      " -0.099599  -0.17312   -0.476     -0.019048  -0.41888   -0.2685\n",
      " -0.65281    0.068773  -0.23881   -1.1784     0.25504    0.61171  ]\n",
      "7562983679033046312\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.378865Z",
     "start_time": "2025-04-23T18:32:00.377180Z"
    }
   },
   "cell_type": "code",
   "source": "print(nlp.vocab.strings[7562983679033046312])",
   "id": "ff544e4c44577b93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.392515Z",
     "start_time": "2025-04-23T18:32:00.390707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(nlp.vocab['Apple'].shape_)\n",
    "print(nlp.vocab['NLP'].shape_)\n",
    "print(nlp.vocab['2023'].shape_)"
   ],
   "id": "f6db1c1d8f1bcf5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xxxxx\n",
      "XXX\n",
      "dddd\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üîπ 4.2 Token Pattern Matching with `Matcher`",
   "id": "982abf1a89565a3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.410121Z",
     "start_time": "2025-04-23T18:32:00.403992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "text = \"Apple is looking to buy a U.K. startup for $1 billion.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Match pattern: a proper noun followed by a verb\n",
    "pattern = [\n",
    "    {\"POS\": \"PROPN\"},\n",
    "    {\"POS\": \"AUX\"},\n",
    "    {\"POS\": \"VERB\"}\n",
    "]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PROPN_VERB_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(\"Match found:\", span.text)"
   ],
   "id": "110f493d3b5decee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found: Apple is looking\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can see available token attributes [here](https://spacy.io/usage/rule-based-matching#adding-patterns-attributes) and [here](https://spacy.io/usage/rule-based-matching#adding-patterns-attributes-extended).",
   "id": "67972b10809c42d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.420240Z",
     "start_time": "2025-04-23T18:32:00.418322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p1 = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "p2 = [\n",
    "    [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}],\n",
    "    [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}]\n",
    "]\n",
    "p3 = [{\"TAG\": {\"REGEX\": \"^V\"}}]"
   ],
   "id": "fbacb25c33dfe91a",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üîπ 4.3 Phrase Matching with PhraseMatcher",
   "id": "1fe464ea63a0bd06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.435454Z",
     "start_time": "2025-04-23T18:32:00.425523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "phrases = [\"natural language processing\", \"deep learning\", \"machine learning\"]\n",
    "patterns = [nlp(text) for text in phrases]\n",
    "\n",
    "matcher.add(\"AI_PHRASES\", patterns)\n",
    "\n",
    "doc = nlp(\"I love natural language processing and machine learning.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Phrase match:\", doc[start:end].text)"
   ],
   "id": "41fafbbaec8a3c22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase match: natural language processing\n",
      "Phrase match: machine learning\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîπ 4.4 Rule-based NER with EntityRuler\n",
    "\n",
    "You can use an `EntityRuler` to create **custom named entities** using patterns."
   ],
   "id": "98ea372e342d8944"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.449108Z",
     "start_time": "2025-04-23T18:32:00.439714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "patterns = [\n",
    "    {\"label\": \"SOFTWARE\", \"pattern\": \"spaCy\"},\n",
    "    {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"openai\"}]},\n",
    "]\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"I use spaCy and OpenAI tools.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"‚Üí\", ent.label_)"
   ],
   "id": "8728ee7f41e8228a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy ‚Üí SOFTWARE\n",
      "OpenAI ‚Üí ORG\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üîÑ Part 5: Processing Pipelines\n",
    "\n",
    "spaCy‚Äôs processing flow is modular and transparent. The `nlp` object you‚Äôve been using is actually a **pipeline of components**, and each one performs a task like tokenization, tagging, parsing, or NER.\n",
    "\n",
    "You can inspect, modify, and even extend this pipeline to add your own logic."
   ],
   "id": "480ab3db895f91aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üîπ 5.1 View the Pipeline",
   "id": "78846744d9d9b5b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.454623Z",
     "start_time": "2025-04-23T18:32:00.453152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the names and order of pipeline components\n",
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ],
   "id": "a761a4d0a45a65e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'entity_ruler', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x13d747a00>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x13d747dc0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x13ee78190>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x13eabae80>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x118996c00>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x11ab0ce40>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x13ee78120>)]\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîπ 5.2 Run Individual Components\n",
    "\n",
    "Each component in the pipeline can be run individually using `nlp.get_pipe()`."
   ],
   "id": "c5f29d49c6838f4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.470859Z",
     "start_time": "2025-04-23T18:32:00.466102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc = nlp(\"OpenAI develops advanced AI models.\")\n",
    "tagger = nlp.get_pipe(\"tagger\")\n",
    "tagger(doc)\n",
    "\n",
    "print([(token.text, token.pos_) for token in doc])"
   ],
   "id": "6543978e0f7b7e54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('OpenAI', 'PROPN'), ('develops', 'VERB'), ('advanced', 'ADJ'), ('AI', 'PROPN'), ('models', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üîπ 5.3 Reordering Components",
   "id": "aa5e08f398869884"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.631103Z",
     "start_time": "2025-04-23T18:32:00.476450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp2 = spacy.load('en_core_web_sm')\n",
    "\n",
    "ner = nlp2.remove_pipe('ner')\n",
    "nlp2.add_pipe('ner', after='parser')\n",
    "\n",
    "print(nlp2.pipe_names)"
   ],
   "id": "680501a00941175d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîπ 5.4 Disable Components Temporarily\n",
    "\n",
    "To speed things up or test specific behavior, you can disable parts of the pipeline:"
   ],
   "id": "b57256f835eb20ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.638924Z",
     "start_time": "2025-04-23T18:32:00.635450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('NLP pipeline before disabling:', nlp.pipe_names)\n",
    "\n",
    "with nlp.select_pipes(disable=[\"ner\", \"parser\"]):\n",
    "    print('NLP pipeline after disabling:', nlp.pipe_names)\n",
    "\n",
    "    doc = nlp(\"Google is based in California.\")\n",
    "    print([(token.text, token.pos_) for token in doc])  # No NER or parsing"
   ],
   "id": "78dce8a24de2ca75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP pipeline before disabling: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'entity_ruler', 'ner']\n",
      "NLP pipeline after disabling: ['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'entity_ruler']\n",
      "[('Google', 'PROPN'), ('is', 'AUX'), ('based', 'VERB'), ('in', 'ADP'), ('California', 'PROPN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîπ 5.5 Add a Custom Component\n",
    "\n",
    "You can insert your own logic into the pipeline using the `@Language.component` decorator."
   ],
   "id": "3fb4592ffb35cfd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.654287Z",
     "start_time": "2025-04-23T18:32:00.647775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "@Language.component(\"print_token_count\")\n",
    "def print_token_count(doc):\n",
    "    print(f\"Processed {len(doc)} tokens.\")\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add it to the end of the pipeline\n",
    "# You can insert your component first=True, last=True, or before='ner', after='parser', etc.\n",
    "nlp.add_pipe(\"print_token_count\", last=True)\n",
    "\n",
    "# Now every time you run `nlp`, the component is triggered\n",
    "doc = nlp(\"This sentence will trigger our custom component.\")"
   ],
   "id": "d2d2a8fa6d7297ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8 tokens.\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üîö Part 6: Summary & Comparison\n",
   "id": "29142dc1140945b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚úÖ What We‚Äôve Learned in This spaCy Section\n",
    "\n",
    "| Topic                           | What We Did                                                                                  |\n",
    "|---------------------------------|----------------------------------------------------------------------------------------------|\n",
    "| **Tokenization & Segmentation** | Used `doc = nlp(text)` and `doc.sents` to extract tokens and sentences                       |\n",
    "| **POS Tagging & Morphology**    | Accessed `token.pos_`, `token.tag_`, and `token.morph`                                       |\n",
    "| **Lemmatization**               | Retrieved `token.lemma_`                                                                     |\n",
    "| **Dependency Parsing**          | Visualized with `displacy.render(..., style='dep')` and inspected `token.dep_`, `token.head` |\n",
    "| **Named Entity Recognition**    | Extracted `doc.ents` and visualized with `displacy.render(..., style='ent')`                 |\n",
    "| **Rule-based Matching**         | Matched patterns using `Matcher`, `PhraseMatcher`, and `EntityRuler`                         |\n",
    "| **Processing Pipelines**        | Inspected `nlp.pipe_names`, disabled components, and added a custom component                |"
   ],
   "id": "2614952f62eb954c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîÅ Comparison: NLTK vs. spaCy\n",
    "\n",
    "| Feature                       | NLTK                                            | spaCy                                         |\n",
    "|-------------------------------|-------------------------------------------------|-----------------------------------------------|\n",
    "| **Language support**          | English; limited others                         | Multilingual; many high-quality models        |\n",
    "| **Tokenization**              | Manual with `punkt`; slower                     | Fast, built-in tokenizer                      |\n",
    "| **POS tagging**               | Classical tagger (`averaged_perceptron_tagger`) | Statistical models; Universal & detailed tags |\n",
    "| **Morphology**                | Basic lemmatization; no morph features          | Rich `.morph` attribute                       |\n",
    "| **Dependency parsing**        | Available but slower                            | Fast, accurate parser                         |\n",
    "| **Named Entity Recognition**  | Rule-based `ne_chunk`; limited accuracy         | ML-based NER + `EntityRuler`                  |\n",
    "| **Rule-based matching**       | Not built-in                                    | `Matcher`, `PhraseMatcher`, `EntityRuler`     |\n",
    "| **Processing pipeline**       | Not modular by default                          | Modular `nlp` pipeline; easy to extend        |\n",
    "| **Word vectors & similarity** | None                                            | Supported in medium/large models              |\n",
    "| **Ease of use**               | Great for learning                              | Great for production and prototyping          |"
   ],
   "id": "75f59a23a0d0788a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> üìå **TL;DR:**\n",
    "> - **NLTK** is ideal for learning classical NLP concepts and quick prototyping.\n",
    "> - **spaCy** excels in performance, modularity, and modern, production-ready NLP workflows."
   ],
   "id": "e26b4ce3b1c016cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üß™ Part 7: Exercises",
   "id": "df5a5b7d9362ddda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîπ 7.1 Write a `preprocess` function\n",
    "\n",
    "Create a Python function with this signature:"
   ],
   "id": "5b280422678756e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:24:30.388223Z",
     "start_time": "2025-04-23T21:24:30.008175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "def preprocess(text: str, nlp: Language) -> list[str]:\n",
    "    \"\"\"\n",
    "    1. Run the text through nlp to get a Doc.\n",
    "    2. For each token, keep only alphabetic tokens that are not stopwords\n",
    "       and whose POS is one of NOUN, VERB, ADJ, or ADV.\n",
    "    3. Return a list of token.lemma_.lower() values.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ],
   "id": "bb66060b6a35c03b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.669131Z",
     "start_time": "2025-04-23T18:32:00.667714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"SpaCy pipelines make it easy to build robust NLP systems.\"\n",
    "print(preprocess(text, nlp))\n",
    "# Expected: ['spacy', 'pipeline', 'easy', 'build', 'robust', 'system']"
   ],
   "id": "366dca85c4870d26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ‚úÖ Answer",
   "id": "b5fc5537b91999e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.679137Z",
     "start_time": "2025-04-23T18:32:00.677280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "def preprocess(text: str, nlp: Language) -> list[str]:\n",
    "    \"\"\"\n",
    "    1. Run the text through nlp to get a Doc.\n",
    "    2. Keep only alphabetic tokens that are not stopwords\n",
    "       and whose POS is one of NOUN, VERB, ADJ, or ADV.\n",
    "    3. Return a list of token.lemma_.lower() values.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    my_list = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha and not token.is_stop and token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV'}:\n",
    "            my_list.append(token.lemma_.lower())\n",
    "\n",
    "    return my_list"
   ],
   "id": "d6fac2842329164f",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.691457Z",
     "start_time": "2025-04-23T18:32:00.686275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"SpaCy pipelines make it easy to build robust NLP systems.\"\n",
    "print(preprocess(text, nlp))"
   ],
   "id": "e08b30121b103bd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 11 tokens.\n",
      "['spacy', 'pipeline', 'easy', 'build', 'robust', 'system']\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîπ 7.2 Turn `preprocess` into a spaCy component\n",
    "Use the `@Language.component` decorator to wrap your function so it can be inserted into `nlp.pipeline`:"
   ],
   "id": "e61bb3c6be41ec7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.899834Z",
     "start_time": "2025-04-23T18:32:00.693231Z"
    }
   },
   "cell_type": "code",
   "source": "nlp = spacy.load(\"en_core_web_sm\")",
   "id": "b63da5c9d764cce1",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.908196Z",
     "start_time": "2025-04-23T18:32:00.904316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "\n",
    "@Language.component(\"simple_preprocessor\")\n",
    "def simple_preprocessor(doc: Doc) -> Doc:\n",
    "    \"\"\"\n",
    "    1. Apply your preprocessing logic to doc\n",
    "    2. Store the result on the Doc, e.g. doc.user_data[\"tokens\"] = [...]\n",
    "    \"\"\"\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add it to the pipeline before \"ner\"\n",
    "nlp.add_pipe(\"simple_preprocessor\", before=\"ner\")"
   ],
   "id": "91c169fe2396281a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.simple_preprocessor(doc: spacy.tokens.doc.Doc) -> spacy.tokens.doc.Doc>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.922047Z",
     "start_time": "2025-04-23T18:32:00.916551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"SpaCy pipelines make it easy to build robust NLP systems.\"\n",
    "doc = nlp(text)\n",
    "print(doc.user_data[\"tokens\"] if \"tokens\" in doc.user_data else \"No tokens found.\")\n",
    "# Expected: ['spacy', 'pipeline', 'easy', 'build', 'robust', 'system']"
   ],
   "id": "3c79596ba88470d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tokens found.\n"
     ]
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ‚úÖ Answer",
   "id": "61f0b05a4aab962f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.937194Z",
     "start_time": "2025-04-23T18:32:00.933001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "@Language.component('simple_preprocessor')\n",
    "def simple_preprocessor(doc):\n",
    "    \"\"\"\n",
    "    1. Apply your preprocessing logic to doc\n",
    "    2. Store the result on the Doc, e.g. doc.user_data[\"tokens\"] = [...]\n",
    "    \"\"\"\n",
    "\n",
    "    my_list = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha and not token.is_stop and token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV'}:\n",
    "            my_list.append(token.lemma_.lower())\n",
    "\n",
    "    doc.user_data['tokens'] = my_list\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add it to the pipeline before \"ner\"\n",
    "nlp.remove_pipe(\"simple_preprocessor\")\n",
    "nlp.add_pipe(\"simple_preprocessor\", before=\"ner\")"
   ],
   "id": "2a59e84321b0d956",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.simple_preprocessor(doc)>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.950600Z",
     "start_time": "2025-04-23T18:32:00.945933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"SpaCy pipelines make it easy to build robust NLP systems.\"\n",
    "doc = nlp(text)\n",
    "print(doc.user_data[\"tokens\"])"
   ],
   "id": "287ffac9d93c1e10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spacy', 'pipeline', 'easy', 'build', 'robust', 'system']\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîπ 7.3 Speed it up by disabling components\n",
    "\n",
    "For pure preprocessing you don‚Äôt need parsing or NER. Use `select_pipes` to disable them:"
   ],
   "id": "43ef8dfb463a9e24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.958787Z",
     "start_time": "2025-04-23T18:32:00.957551Z"
    }
   },
   "cell_type": "code",
   "source": "# you code here",
   "id": "afa5b84d7b2aa803",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ‚úÖ Answer",
   "id": "57471f44a135cdb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.969702Z",
     "start_time": "2025-04-23T18:32:00.966185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with nlp.select_pipes(disable=['parser', 'ner']):\n",
    "    doc_fast = nlp(text)\n",
    "    fast_tokens = doc_fast.user_data['tokens']\n",
    "\n",
    "print(fast_tokens)"
   ],
   "id": "ce05800f0d0db4c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spacy', 'pipeline', 'easy', 'build', 'robust', 'system']\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîπ 7.4 Challenge\n",
    "\n",
    "- Extend your preprocessor to also remove tokens shorter than 3 characters.\n",
    "- Measure the time difference between running with and without disabling ‚Äúparser‚Äù and ‚Äúner.‚Äù\n",
    "- Add an optional argument to your component so users can choose which POS tags to keep."
   ],
   "id": "128eaabd3c60feb1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ‚úÖ Answer",
   "id": "6409bd16133b0832"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.979106Z",
     "start_time": "2025-04-23T18:32:00.972422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@Language.component('simple_preprocessor2')\n",
    "def simple_preprocessor2(doc):\n",
    "    my_list = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha and not token.is_stop and token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV'} and len(token) >= 5:\n",
    "            my_list.append(token.lemma_.lower())\n",
    "\n",
    "    doc.user_data['tokens2'] = my_list\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Replace the first preprocessor\n",
    "nlp.remove_pipe('simple_preprocessor')\n",
    "nlp.add_pipe('simple_preprocessor2', before='ner')\n",
    "\n",
    "# Test\n",
    "doc2 = nlp(text)\n",
    "print(doc2.user_data['tokens2'])"
   ],
   "id": "af2cdbb815fe938e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spacy', 'pipeline', 'build', 'robust', 'system']\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:00.989820Z",
     "start_time": "2025-04-23T18:32:00.983337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "# Full pipeline\n",
    "start_full = time.perf_counter()\n",
    "doc_full = nlp(text)\n",
    "full_tokens = doc_full.user_data['tokens2']\n",
    "time_full = time.perf_counter() - start_full\n",
    "print(\"Full pipeline tokens:\", full_tokens)\n",
    "\n",
    "# Disable parser + ner\n",
    "start_fast = time.perf_counter()\n",
    "with nlp.select_pipes(disable=['parser', 'ner']):\n",
    "    doc_fast = nlp(text)\n",
    "    fast_tokens = doc_fast.user_data['tokens2']\n",
    "time_fast = time.perf_counter() - start_fast\n",
    "print(\"Fast pipeline tokens:\", fast_tokens)\n",
    "\n",
    "print(f'Full pipeline: {time_full:.6f}s, Without parser+ner: {time_fast:.6f}s')"
   ],
   "id": "cf0b32919a95a0f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full pipeline tokens: ['spacy', 'pipeline', 'build', 'robust', 'system']\n",
      "Fast pipeline tokens: ['spacy', 'pipeline', 'build', 'robust', 'system']\n",
      "Full pipeline: 0.003256s, Without parser+ner: 0.001307s\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:01.007774Z",
     "start_time": "2025-04-23T18:32:00.993792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "@Language.factory(\n",
    "    'advanced_preprocessor',\n",
    "    default_config={'pos_to_keep': ['NOUN', 'VERB'], 'min_length': 4}\n",
    ")\n",
    "# ValueError: [E964] The pipeline component factory for 'advanced_preprocessor' needs to have the following named arguments, which are passed in by spaCy:\n",
    "# - nlp: receives the current nlp object and lets you access the vocab\n",
    "# - name: the name of the component instance, can be used to identify the component, output losses etc.\n",
    "def create_advanced_preprocessor(nlp, name, pos_to_keep, min_length):\n",
    "    def advanced_preprocessor(doc):\n",
    "        my_list = []\n",
    "        for token in doc:\n",
    "            if token.is_alpha and not token.is_stop and token.pos_ in set(pos_to_keep) and len(token) >= min_length:\n",
    "                my_list.append(token.lemma_.lower())\n",
    "\n",
    "        doc.user_data['advanced_tokens'] = my_list\n",
    "        return doc\n",
    "\n",
    "    return advanced_preprocessor\n",
    "\n",
    "\n",
    "# Add to pipeline\n",
    "nlp.add_pipe('advanced_preprocessor', before='ner')\n",
    "\n",
    "# Test\n",
    "doc3 = nlp(text)\n",
    "print(doc3.user_data['advanced_tokens'])"
   ],
   "id": "6f91c259f99f464d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pipeline', 'build', 'system']\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T18:32:01.014061Z",
     "start_time": "2025-04-23T18:32:01.012808Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d1b78762a42bba9e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
