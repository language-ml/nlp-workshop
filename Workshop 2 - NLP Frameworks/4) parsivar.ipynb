{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src='https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png' width=150 height=150> <br>\n",
    "<font color=0F5298 size=6>\n",
    "Natural Language Processing<br>\n",
    "<font color=2565AE size=4>\n",
    "Computer Engineering Department<br>\n",
    "Spring 2025<br>\n",
    "<font color=3C99D size=4>\n",
    "Workshop 1 - NLP Frameworks - Parsivar<br>\n",
    "<font color=696880 size=3>\n",
    "<a href='https://language.ml'>https://language.ml</a><br>\n",
    "info [AT] language [dot] ml"
   ],
   "id": "99ace3b6f559ac3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸ“– Part 1: Introduction\n",
    "\n",
    "## â“ What is Parsivar? ([GitHub](https://github.com/ICTRC/Parsivar))\n",
    "\n",
    "**Parsivar** is an open-source Python library built specifically for **Persian (Farsi) NLP**.\n",
    "It offers ready-made components for:\n",
    "\n",
    "- **Normalization** and orthographic fixes\n",
    "- **Sentence / word tokenization**\n",
    "- **Stemming** (rule-based)\n",
    "- **Part-of-speech tagging** (HMM/Wapiti)\n",
    "- **Dependency parsing** (UD-style)\n",
    "- **Spell checking / autocorrection**\n",
    "\n",
    "*(Parsivar does **not** ship a lemmatizer or NER in its current release.)*\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… When to Use Parsivar\n",
    "\n",
    "- You need a **quick, self-contained Persian NLP** pipeline in pure Python.\n",
    "- You want features **not bundled in Hazm** (e.g. built-in NER, sentiment).\n",
    "- You prefer **rule + ML hybrids** instead of heavier deep-learning models.\n",
    "- Youâ€™re preprocessing Persian text for classical ML or linguistic analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš« When Not to Use Parsivar\n",
    "\n",
    "- You need **transformer-based** embeddings or state-of-the-art accuracy (use ðŸ¤— Hugging Face models).\n",
    "- You require **dependency parsing** (Parsivar doesnâ€™t provide one).\n",
    "- Youâ€™re building **multilingual** pipelines (Parsivar is Persian-only).\n",
    "- You need **industrial throughput**â€”for very large-scale, spaCy with custom Persian models is faster."
   ],
   "id": "98dacdc744a17d41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## âš™ï¸ Installation & Setup",
   "id": "2da01515538c4e99"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T21:44:23.333911Z",
     "start_time": "2025-04-23T21:44:11.595948Z"
    }
   },
   "source": "!pip install parsivar",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting parsivar\r\n",
      "  Downloading parsivar-0.2.3.1-py3-none-any.whl.metadata (242 bytes)\r\n",
      "Requirement already satisfied: nltk>=3.6.6 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from parsivar) (3.9.1)\r\n",
      "Requirement already satisfied: click in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from nltk>=3.6.6->parsivar) (8.1.8)\r\n",
      "Requirement already satisfied: joblib in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from nltk>=3.6.6->parsivar) (1.4.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from nltk>=3.6.6->parsivar) (2024.11.6)\r\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from nltk>=3.6.6->parsivar) (4.67.1)\r\n",
      "Downloading parsivar-0.2.3.1-py3-none-any.whl (18.0 MB)\r\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m18.0/18.0 MB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\r\n",
      "\u001B[?25hInstalling collected packages: parsivar\r\n",
      "Successfully installed parsivar-0.2.3.1\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ’¡ Part 2: Preprocessing\n",
    "\n",
    "Weâ€™ll start by **normalizing** raw Persian text and then splitting it into **sentences** and **words** with Parsivar."
   ],
   "id": "d156592c874c2fa6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ“¦ Imports",
   "id": "5ccedc8187926827"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:56:21.140019Z",
     "start_time": "2025-04-23T21:56:21.137846Z"
    }
   },
   "cell_type": "code",
   "source": "from parsivar import Normalizer, Tokenizer",
   "id": "3e21595536fe0df8",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ”¹ 2.1 Normalization",
   "id": "e56ca2a21886596d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:26:24.787807Z",
     "start_time": "2025-04-23T22:26:24.784298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø§ÛŒØ³Ù†Ø§ Ø³Ù…ÛŒÙ†Ø§Ø±Ù‡Ø§ ÛŒ Ø´ÛŒÙ…ÛŒ Ø¢Ù„ÛŒ Ø§Ø² Ø§Ù…Ø±ÙˆØ² Û±Û± Ø´Ù‡Ø±ÛŒÙˆØ± Û±Û³Û¹Û¶ Ø¯Ø± Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø¹Ù„Ù… Ùˆ ØµÙ†Ø¹Øª Ø§ÛŒØ±Ø§Ù† Ø¢ØºØ§Ø² Ø¨Ù‡ Ú©Ø§Ø± Ú©Ø±Ø¯Ù†Ø¯. Ø§ÛŒÙ† Ø³Ù…ÛŒÙ†Ø§Ø± ØªØ§ Û±Û³ Ø´Ù‡Ø±ÛŒÙˆØ± Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒ ÛŒØ§Ø¨Ø¯.\"\n",
    "normalizer = Normalizer()\n",
    "print(normalizer.normalize(text))"
   ],
   "id": "50fb6c95b887c972",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø§ÛŒØ³Ù†Ø§ Ø³Ù…ÛŒÙ†Ø§Ø±Ù‡Ø§ ÛŒ Ø´ÛŒÙ…ÛŒ Ø¢Ù„ÛŒ Ø§Ø² Ø§Ù…Ø±ÙˆØ² 11 Ø´Ù‡Ø±ÛŒÙˆØ± 1396 Ø¯Ø± Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø¹Ù„Ù… Ùˆ ØµÙ†Ø¹Øª Ø§ÛŒØ±Ø§Ù† Ø¢ØºØ§Ø² Ø¨Ù‡ Ú©Ø§Ø± Ú©Ø±Ø¯Ù†Ø¯ . Ø§ÛŒÙ† Ø³Ù…ÛŒÙ†Ø§Ø± ØªØ§ 13 Ø´Ù‡Ø±ÛŒÙˆØ± Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯ .\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:26:26.950303Z",
     "start_time": "2025-04-23T22:26:26.920543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "normalizer = Normalizer(statistical_space_correction=True)\n",
    "print(normalizer.normalize(text))"
   ],
   "id": "6faac56f35291f93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø§ÛŒØ³Ù†Ø§ Ø³Ù…ÛŒÙ†Ø§Ø±Ù‡Ø§â€ŒÛŒ Ø´ÛŒÙ…ÛŒ Ø¢Ù„ÛŒ Ø§Ø² Ø§Ù…Ø±ÙˆØ² 11 Ø´Ù‡Ø±ÛŒÙˆØ± 1396 Ø¯Ø± Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø¹Ù„Ù… Ùˆ ØµÙ†Ø¹Øª Ø§ÛŒØ±Ø§Ù† Ø¢ØºØ§Ø² Ø¨Ù‡ Ú©Ø§Ø± Ú©Ø±Ø¯Ù†Ø¯ . Ø§ÛŒÙ† Ø³Ù…ÛŒÙ†Ø§Ø± ØªØ§ 13 Ø´Ù‡Ø±ÛŒÙˆØ± Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯ . \n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:26:28.844612Z",
     "start_time": "2025-04-23T22:26:28.826739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "normalizer = Normalizer(date_normalizing_needed=True)\n",
    "print(normalizer.normalize(text))"
   ],
   "id": "95be8298bb60aa26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø§ÛŒØ³Ù†Ø§ Ø³Ù…ÛŒÙ†Ø§Ø±Ù‡Ø§ ÛŒ Ø´ÛŒÙ…ÛŒ Ø¢Ù„ÛŒ Ø§Ø² Ø§Ù…Ø±ÙˆØ² y1396m6d11 Ø¯Ø± Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø¹Ù„Ù… Ùˆ ØµÙ†Ø¹Øª Ø§ÛŒØ±Ø§Ù† Ø¢ØºØ§Ø² Ø¨Ù‡ Ú©Ø§Ø± Ú©Ø±Ø¯Ù†Ø¯ . Ø§ÛŒÙ† Ø³Ù…ÛŒÙ†Ø§Ø± ØªØ§ y0m6d13 Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯ .\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:26:29.224591Z",
     "start_time": "2025-04-23T22:26:29.206451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "normalizer = Normalizer(pinglish_conversion_needed=True)\n",
    "print(normalizer.normalize(\"farda asman abri ast.\"))"
   ],
   "id": "bb225b9c557e7e5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÙØ±Ø¯Ø§ Ø§Ø³Ù…Ø§Ù† Ø§Ø¨Ø±ÛŒ Ø§Ø³Øª .\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ”¹ 2.2 Sentence Tokenization",
   "id": "fa397620c870e8b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:26:34.318606Z",
     "start_time": "2025-04-23T22:26:34.313165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer()\n",
    "normalizer = Normalizer()\n",
    "sentences = tokenizer.tokenize_sentences(normalizer.normalize(text))\n",
    "\n",
    "for sent in sentences:\n",
    "    print(sent)"
   ],
   "id": "a7398b1f4bf5effc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø§ÛŒØ³Ù†Ø§ Ø³Ù…ÛŒÙ†Ø§Ø±Ù‡Ø§ ÛŒ Ø´ÛŒÙ…ÛŒ Ø¢Ù„ÛŒ Ø§Ø² Ø§Ù…Ø±ÙˆØ² 11 Ø´Ù‡Ø±ÛŒÙˆØ± 1396 Ø¯Ø± Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø¹Ù„Ù… Ùˆ ØµÙ†Ø¹Øª Ø§ÛŒØ±Ø§Ù† Ø¢ØºØ§Ø² Ø¨Ù‡ Ú©Ø§Ø± Ú©Ø±Ø¯Ù†Ø¯  .\n",
      " Ø§ÛŒÙ† Ø³Ù…ÛŒÙ†Ø§Ø± ØªØ§ 13 Ø´Ù‡Ø±ÛŒÙˆØ± Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯  .\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ”¹ 2.3 Word Tokenization",
   "id": "a35897298e966687"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:26:35.674762Z",
     "start_time": "2025-04-23T22:26:35.671619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for sent in sentences:\n",
    "    words = tokenizer.tokenize_words(normalizer.normalize(sent))\n",
    "    print(words)"
   ],
   "id": "d911d6181ca1a442",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ø¨Ù‡', 'Ú¯Ø²Ø§Ø±Ø´', 'Ø§ÛŒØ³Ù†Ø§', 'Ø³Ù…ÛŒÙ†Ø§Ø±Ù‡Ø§', 'ÛŒ', 'Ø´ÛŒÙ…ÛŒ', 'Ø¢Ù„ÛŒ', 'Ø§Ø²', 'Ø§Ù…Ø±ÙˆØ²', '11', 'Ø´Ù‡Ø±ÛŒÙˆØ±', '1396', 'Ø¯Ø±', 'Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡', 'Ø¹Ù„Ù…', 'Ùˆ', 'ØµÙ†Ø¹Øª', 'Ø§ÛŒØ±Ø§Ù†', 'Ø¢ØºØ§Ø²', 'Ø¨Ù‡', 'Ú©Ø§Ø±', 'Ú©Ø±Ø¯Ù†Ø¯', '.']\n",
      "['Ø§ÛŒÙ†', 'Ø³Ù…ÛŒÙ†Ø§Ø±', 'ØªØ§', '13', 'Ø´Ù‡Ø±ÛŒÙˆØ±', 'Ø§Ø¯Ø§Ù…Ù‡', 'Ù…ÛŒ\\u200cÛŒØ§Ø¨Ø¯', '.']\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸ§  Part 3: Morphology â€” Stemming\n",
    "\n",
    "Parsivar provides a rule-based **stemmer** but **does not** include a lemmatizer.\n",
    "\n",
    "If you need full lemmatization youâ€™ll have to integrate another tool (e.g. Hazmâ€™s `Lemmatizer`)."
   ],
   "id": "c876a86eea65177c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ“¦ Imports",
   "id": "87363392a2379e02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:56:42.322959Z",
     "start_time": "2025-04-23T21:56:42.320954Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": "from parsivar import FindStems, Tokenizer",
   "id": "d6eba4a69f110b54",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ”¹ 3.1 Stemming",
   "id": "8a6be2a97d0ce506"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:26:59.864359Z",
     "start_time": "2025-04-23T22:26:59.858041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stemmer = FindStems()\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "sentence = 'Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯Ù… Ø±Ø§ Ø¨Ù‡ Ø¯ÙˆØ³ØªØ§Ù† Ø®ÙˆØ¨Ù… Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù….'\n",
    "tokens = tokenizer.tokenize_words(sentence)\n",
    "\n",
    "stems = [stemmer.convert_to_stem(t) for t in tokens]\n",
    "print(tokens)\n",
    "print(stems)"
   ],
   "id": "83df020bdb4ef95d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ú©ØªØ§Ø¨\\u200cÙ‡Ø§ÛŒ', 'Ø¬Ø¯ÛŒØ¯Ù…', 'Ø±Ø§', 'Ø¨Ù‡', 'Ø¯ÙˆØ³ØªØ§Ù†', 'Ø®ÙˆØ¨Ù…', 'Ù†Ø´Ø§Ù†', 'Ø¯Ø§Ø¯Ù….']\n",
      "['Ú©ØªØ§Ø¨', 'Ø¬Ø¯ÛŒØ¯', 'Ø±Ø§', 'Ø¨Ù‡', 'Ø¯ÙˆØ³Øª', 'Ø®ÙˆØ¨', 'Ù†Ø´Ø§Ù†', 'Ø¯Ø§Ø¯Ù….']\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# âœï¸ Part 4: POS Tagging & Chunking",
   "id": "487e6974cccbdf40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ“¦ Imports\n",
    "\n",
    "Parsivarâ€™s HMMâ€based POS tagger uses the **Wapiti** library under the hood. Before you run the tagger, install both Parsivar and its Wapiti binding."
   ],
   "id": "49f8e6c56bb6e6aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:00:02.978353Z",
     "start_time": "2025-04-23T21:59:57.433815Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install libwapiti",
   "id": "47c5fe6cf8e47ffc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting libwapiti\r\n",
      "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: six in /opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages (from libwapiti) (1.17.0)\r\n",
      "Building wheels for collected packages: libwapiti\r\n",
      "  Building wheel for libwapiti (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp310-cp310-macosx_11_0_arm64.whl size=49039 sha256=6d4d50bf5d5f716bde41e58399bb2506bc54152c3a911c822257ba8ecd035392\r\n",
      "  Stored in directory: /Users/AmirMohammad/Library/Caches/pip/wheels/9f/cb/30/fef48ecac051e433987eccdb5682900b4c00d44a4bcd4d4ec8\r\n",
      "Successfully built libwapiti\r\n",
      "Installing collected packages: libwapiti\r\n",
      "Successfully installed libwapiti-0.2.1\r\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:09:01.701931Z",
     "start_time": "2025-04-23T22:09:01.700013Z"
    }
   },
   "cell_type": "code",
   "source": "from parsivar import Tokenizer, POSTagger, FindChunks",
   "id": "7ddb67aa86227d65",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ”¹ 4.1 Tokenization & POS Tagging",
   "id": "261d9f1268b61d6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:04:23.655301Z",
     "start_time": "2025-04-23T22:04:23.378889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate tokenizer and tagger (use \"wapiti\" or \"stanford\")\n",
    "tokenizer = Tokenizer()\n",
    "tagger = POSTagger(tagging_model=\"wapiti\")\n",
    "\n",
    "sentence = 'Ù…Ù† Ø¯ÙˆØ³Øª Ø¯Ø§Ø±Ù… Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø¨Ù‡ØªØ± ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ù….'\n",
    "\n",
    "tokens = tokenizer.tokenize_words(sentence)\n",
    "tags = tagger.parse(tokens)\n",
    "\n",
    "tags"
   ],
   "id": "92ef6a86b2e1ea02",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ù…Ù†', 'PRO'),\n",
       " ('Ø¯ÙˆØ³Øª', 'N_SING'),\n",
       " ('Ø¯Ø§Ø±Ù…', 'V_PRS'),\n",
       " ('Ø²Ø¨Ø§Ù†', 'N_SING'),\n",
       " ('ÙØ§Ø±Ø³ÛŒ', 'ADJ'),\n",
       " ('Ø±Ø§', 'CLITIC'),\n",
       " ('Ø¨Ù‡ØªØ±', 'ADJ_CMPR'),\n",
       " ('ÛŒØ§Ø¯', 'N_SING'),\n",
       " ('Ø¨Ú¯ÛŒØ±Ù….', 'V_SUB')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“‹ Parsivar POS Tagset Reference\n",
    "\n",
    "| Tag      | Category     | Meaning                |\n",
    "|----------|--------------|------------------------|\n",
    "| N_SING   | Noun         | Singular noun          |\n",
    "| N_PLUR   | Noun         | Plural noun            |\n",
    "| V_PRS    | Verb         | Present-tense verb     |\n",
    "| V_PST    | Verb         | Past-tense verb        |\n",
    "| V_IMP    | Verb         | Imperative verb        |\n",
    "| ADJ      | Adjective    | Adjective              |\n",
    "| ADV      | Adverb       | Adverb                 |\n",
    "| PRON     | Pronoun      | Pronoun                |\n",
    "| DET      | Determiner   | Determiner             |\n",
    "| NUM      | Numeral      | Numeral                |\n",
    "| PREP     | Preposition  | Preposition            |\n",
    "| CONJ     | Conjunction  | Conjunction            |\n",
    "| PUNC     | Punctuation  | Punctuation mark       |"
   ],
   "id": "c06632fbfb698258"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ”¹ 4.2 Phrase Chunking",
   "id": "e8ff7701b13cc4ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:08:22.484939Z",
     "start_time": "2025-04-23T23:08:22.481715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunker = FindChunks()\n",
    "chunk_tree = chunker.chunk_sentence(tags)"
   ],
   "id": "5ea5b4fd3af5f7e4",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:08:56.456853Z",
     "start_time": "2025-04-23T23:08:56.454401Z"
    }
   },
   "cell_type": "code",
   "source": "print(chunk_tree)",
   "id": "b8b3441b9d249023",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Ù…Ù†/PRO\n",
      "  (VP (VP Ø¯ÙˆØ³Øª/N_SING Ø¯Ø§Ø±Ù…/V_PRS))\n",
      "  (NP Ø²Ø¨Ø§Ù†/N_SING ÙØ§Ø±Ø³ÛŒ/ADJ)\n",
      "  Ø±Ø§/CLITIC\n",
      "  Ø¨Ù‡ØªØ±/ADJ_CMPR\n",
      "  (VP (VP ÛŒØ§Ø¯/N_SING Ø¨Ú¯ÛŒØ±Ù…./V_SUB)))\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:09:00.143341Z",
     "start_time": "2025-04-23T23:09:00.141015Z"
    }
   },
   "cell_type": "code",
   "source": "print(chunker.convert_nestedtree2rawstring(chunk_tree))",
   "id": "fbda5df643255374",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù…Ù† [Ø¯ÙˆØ³Øª Ø¯Ø§Ø±Ù… VP] [Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ NP] Ø±Ø§ Ø¨Ù‡ØªØ± [ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ù…. VP]\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> `NP` denotes noun phrases, `VP` verb phrases, giving you easy access to phrase-level units for further analysis.",
   "id": "ce5122cb47c7fb37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ðŸŒ³ Part 5: Dependency Parsing",
   "id": "e7bc2312c75c4215"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ“¦ Imports",
   "id": "5ad3be721ba8816c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:30:22.695308Z",
     "start_time": "2025-04-23T22:30:22.693429Z"
    }
   },
   "cell_type": "code",
   "source": "from parsivar import Tokenizer, DependencyParser",
   "id": "392aa5b12d1bb05a",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ”¹ 5.1 Parse Sentences",
   "id": "4cef2862afa5e441"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:32:43.680804Z",
     "start_time": "2025-04-23T22:32:42.729701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer()\n",
    "parser = DependencyParser()\n",
    "\n",
    "text = \"Ø¨Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø§ÛŒØ³Ù†Ø§ Ø³Ù…ÛŒÙ†Ø§Ø±Ù‡Ø§ ÛŒ Ø´ÛŒÙ…ÛŒ Ø¢Ù„ÛŒ Ø§Ø² Ø§Ù…Ø±ÙˆØ² Û±Û± Ø´Ù‡Ø±ÛŒÙˆØ± Û±Û³Û¹Û¶ Ø¯Ø± Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø¹Ù„Ù… Ùˆ ØµÙ†Ø¹Øª Ø§ÛŒØ±Ø§Ù† Ø¢ØºØ§Ø² Ø¨Ù‡ Ú©Ø§Ø± Ú©Ø±Ø¯Ù†Ø¯. Ø§ÛŒÙ† Ø³Ù…ÛŒÙ†Ø§Ø± ØªØ§ Û±Û³ Ø´Ù‡Ø±ÛŒÙˆØ± Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒ ÛŒØ§Ø¨Ø¯.\"\n",
    "\n",
    "sentences = tokenizer.tokenize_sentences(text)\n",
    "parsed_graphs = parser.parse_sents(sentences)\n",
    "\n",
    "for g in parsed_graphs:\n",
    "    print(g.tree())"
   ],
   "id": "a3e43be2c08aa76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Ú©Ø±Ø¯Ù†Ø¯\n",
      "  (Ø¨Ù‡ (Ú¯Ø²Ø§Ø±Ø´ (Ø§ÛŒØ³Ù†Ø§ (Ø³Ù…ÛŒÙ†Ø§Ø±Ù‡Ø§ (ÛŒ (Ø´ÛŒÙ…ÛŒ Ø¢Ù„ÛŒ))))))\n",
      "  (Ø§Ø² Ø§Ù…Ø±ÙˆØ²)\n",
      "  (Ø´Ù‡Ø±ÛŒÙˆØ± Û±Û±)\n",
      "  (Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ (Ø¯Ø± Û±Û³Û¹Û¶) (Ø¹Ù„Ù… (Ùˆ (ØµÙ†Ø¹Øª Ø§ÛŒØ±Ø§Ù†))))\n",
      "  Ø¢ØºØ§Ø²\n",
      "  (Ø¨Ù‡ Ú©Ø§Ø±)\n",
      "  .)\n",
      "(ÛŒØ§Ø¨Ø¯ (Ø³Ù…ÛŒÙ†Ø§Ø± Ø§ÛŒÙ†) (ØªØ§ (Ø´Ù‡Ø±ÛŒÙˆØ± Û±Û³)) Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒ .)\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ðŸ› ï¸ Part 6: Spell Correction",
   "id": "9c032affa30edf0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ“¦ Imports",
   "id": "81db6f055404d077"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:33:43.111287Z",
     "start_time": "2025-04-23T22:33:43.108625Z"
    }
   },
   "cell_type": "code",
   "source": "from parsivar import SpellCheck",
   "id": "7b15923ef351a8c8",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ”¹ 6.1 Correct Misspellings and Spacing\n",
    "\n",
    "To use the spell checker module download it's resources from [here](https://www.dropbox.com/s/tlyvnzv1ha9y1kl/spell.zip?dl=0) and after extraction copy the `spell/` directory to `parsivar/resource` -><br>\n",
    "`FileNotFoundError: [Errno 2] No such file or directory: '/opt/homebrew/anaconda3/envs/Jupyter310/lib/python3.10/site-packages/parsivar/resource/spell/mybigram_lm.pckl'`"
   ],
   "id": "6091f04a64d180ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T22:42:16.205329Z",
     "start_time": "2025-04-23T22:42:14.888543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spell = SpellCheck()\n",
    "\n",
    "bad_sentence = \"Ù†Ù…Ø§Ø²Ú¯Ø°Ø§Ø±Ø§Ù† ÙˆØ§Ø±Ø¯ Ù…Ø³Ù„ÛŒ Ø´Ø¯Ù†Ø¯.\"\n",
    "fixed_sentence = spell.spell_corrector(bad_sentence)\n",
    "\n",
    "print(bad_sentence)\n",
    "print(fixed_sentence)"
   ],
   "id": "601126b664ec391e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù†Ù…Ø§Ø²Ú¯Ø°Ø§Ø±Ø§Ù† ÙˆØ§Ø±Ø¯ Ù…Ø³Ù„ÛŒ Ø´Ø¯Ù†Ø¯.\n",
      "Ù†Ù…Ø§Ø²Ú¯Ø²Ø§Ø±Ø§Ù† ÙˆØ§Ø±Ø¯ Ù…ØµÙ„ÛŒ Ø´Ø¯Ù†Ø¯ .\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸ”š Part 7: Summary & Comparison (Parsivar vs. Hazm)\n",
    "\n",
    "| Feature / Task               | **Hazm**                                          | **Parsivar**                                        |\n",
    "|------------------------------|---------------------------------------------------|-----------------------------------------------------|\n",
    "| **Normalization**            | âœ” Extensive, configurable                         | âœ” Good, statistical space correction                |\n",
    "| **Spell Correction**         | âœ–                                                 | âœ” `SpellCheck` (needs LM files)                     |\n",
    "| **Sentence Tokenizer**       | âœ” `sent_tokenize`                                 | âœ” `Tokenizer.tokenize_sentences`                    |\n",
    "| **Word Tokenizer**           | âœ” `word_tokenize`                                 | âœ” `Tokenizer.tokenize_words`                        |\n",
    "| **Stemming**                 | âœ” `Stemmer`                                       | âœ” `FindStems`                                       |\n",
    "| **Lemmatization**            | âœ” `Lemmatizer`                                    | âœ– (not included)                                    |\n",
    "| **POS Tagging**              | âœ” Bijankhan tagger                                | âœ” HMM/Wapiti tagger                                 |\n",
    "| **Chunker**                  | âœ” `Chunker(model='chunker.model')`                | âœ” `FindChunks`                                      |\n",
    "| **Dependency Parsing**       | âœ” MaltParser wrapper (needs Java)                 | âœ” Pure-Python UD parser                             |\n",
    "| **Word/Sent Embeddings**     | âœ” `WordEmbedding` (fastText / word2vec, etc.)     | âœ–                                                   |\n",
    "| **Named-Entity Recognition** | Optional (works if you supply a trained model)    | âœ– (no built-in support)                             |\n",
    "| **Installation weight**      | Light (Python + optional Java)                    | Light (Python + optional Wapiti)                    |\n",
    "| **Best for â€¦**               | Rich morphology, chunking, embeddings, custom NER | Quick normalization + spell-fix + Java-free parsing |"
   ],
   "id": "a17e3009f66b4c22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **Hazm** now covers **chunking** and **word/sentence embeddings** (fastText/word2vec).\n",
    "  It can also run NER **if you point it to a trained model**, but ships none by default.\n",
    "- **Parsivar** offers built-in **spell correction**, a pure-Python dependency parser, and chunking, but **no lemmatizer, embeddings, or NER**.\n",
    "- Choose **Hazm** for deeper linguistic pipelines (lemmas, embeddings, chunker, optional NER).\n",
    "- Choose **Parsivar** when you need fast spell-correction and a Java-free dependency parser, accepting the absence of lemmatizer/embeddings/NER."
   ],
   "id": "861daac821f708bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ§ª Part 8: Integrated Pre-processing Pipeline (Parsivar Style)\n",
    "\n",
    "Youâ€™ll create a **`preprocess_parsivar`** function, wrap it in a class, and compare speed with spell-correction ON vs. OFF."
   ],
   "id": "fa60c056099f546c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ“¦ Imports (local to this part)",
   "id": "af365c5fc91afdf8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T00:27:44.662009Z",
     "start_time": "2025-04-24T00:27:44.660287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from parsivar import SpellCheck, Normalizer, Tokenizer, FindStems\n",
    "import string, time"
   ],
   "id": "88802018cdb0579b",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ”¹ 8.1 preprocess_parsivar Function",
   "id": "9471153d5a15257f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T00:29:44.296233Z",
     "start_time": "2025-04-24T00:29:43.051060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spell = SpellCheck()\n",
    "normalizer = Normalizer(statistical_space_correction=True)\n",
    "tokenizer = Tokenizer()\n",
    "stemmer = FindStems()\n",
    "PUNCT = set(string.punctuation + 'ØŸØŒØ›Â«Â»â€¦')\n",
    "\n",
    "\n",
    "def preprocess_parsivar(\n",
    "        text: str,\n",
    "        fix_spelling: bool = True,\n",
    "        use_stemmer: bool = False\n",
    ") -> list[str]:\n",
    "    \"\"\"Return a list of clean (optionally stemmed) tokens.\"\"\"\n",
    "    # your code here\n",
    "    pass"
   ],
   "id": "dd20906be4e4090d",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T00:29:46.107891Z",
     "start_time": "2025-04-24T00:29:46.105360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = \"Ù†Ù…Ø§Ø²Ú¯Ø°Ø§Ø±Ø§Ù† ÙˆØ§Ø±Ø¯ Ù…Ø³Ù„ÛŒ Ø´Ø¯Ù†Ø¯.\"\n",
    "print(preprocess_parsivar(sample))\n",
    "# Expected:\n",
    "# ['Ù†Ù…Ø§Ø²Ú¯Ø²Ø§Ø±Ø§Ù†', 'ÙˆØ§Ø±Ø¯', 'Ù…ØµÙ„ÛŒ', 'Ø´Ø¯Ù†Ø¯']"
   ],
   "id": "fb0e844bdd4a9a49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### âœ… Answer",
   "id": "3cc8d96ba1cc418e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T00:29:52.494398Z",
     "start_time": "2025-04-24T00:29:51.168744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spell = SpellCheck()\n",
    "normalizer = Normalizer(statistical_space_correction=True)\n",
    "tokenizer = Tokenizer()\n",
    "stemmer = FindStems()\n",
    "PUNCT = set(string.punctuation + 'ØŸØŒØ›Â«Â»â€¦')\n",
    "\n",
    "\n",
    "def preprocess_parsivar(\n",
    "        text: str,\n",
    "        fix_spelling: bool = True,\n",
    "        use_stemmer: bool = False\n",
    ") -> list[str]:\n",
    "    \"\"\"Return a list of clean (optionally stemmed) tokens.\"\"\"\n",
    "    if fix_spelling:\n",
    "        text = spell.spell_corrector(text)\n",
    "    text = normalizer.normalize(text)\n",
    "\n",
    "    tokens = []\n",
    "    for sent in tokenizer.tokenize_sentences(text):\n",
    "        for tok in tokenizer.tokenize_words(sent):\n",
    "            if tok in PUNCT:\n",
    "                continue\n",
    "            tokens.append(tok)\n",
    "\n",
    "    if use_stemmer:\n",
    "        tokens = [stemmer.convert_to_stem(t) for t in tokens]\n",
    "\n",
    "    return [t.lower() for t in tokens]"
   ],
   "id": "df1afa6c02fa5f75",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T00:29:52.500823Z",
     "start_time": "2025-04-24T00:29:52.496874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = \"Ù†Ù…Ø§Ø²Ú¯Ø°Ø§Ø±Ø§Ù† ÙˆØ§Ø±Ø¯ Ù…Ø³Ù„ÛŒ Ø´Ø¯Ù†Ø¯.\"\n",
    "print(preprocess_parsivar(sample))"
   ],
   "id": "cff216a32d6f82a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ù†Ù…Ø§Ø²Ú¯Ø²Ø§Ø±Ø§Ù†', 'ÙˆØ§Ø±Ø¯', 'Ù…ØµÙ„ÛŒ', 'Ø´Ø¯Ù†Ø¯']\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ”¹ 8.2 ParsivarPreprocessor Class",
   "id": "f3b20cf90797886e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T00:30:13.785877Z",
     "start_time": "2025-04-24T00:30:13.782916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ParsivarPreprocessor:\n",
    "    def __init__(self, fix_spelling=True, use_stemmer=False):\n",
    "        self.fix_spelling = fix_spelling\n",
    "        self.use_stemmer = use_stemmer\n",
    "\n",
    "    def __call__(self, text: str) -> list[str]:\n",
    "        return preprocess_parsivar(\n",
    "            text,\n",
    "            fix_spelling=self.fix_spelling,\n",
    "            use_stemmer=self.use_stemmer\n",
    "        )\n",
    "\n",
    "    def batch(self, docs):\n",
    "        return [self(d) for d in docs]"
   ],
   "id": "aa7397664f69cd0c",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T00:30:14.309361Z",
     "start_time": "2025-04-24T00:30:14.302150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pp = ParsivarPreprocessor(fix_spelling=True, use_stemmer=True)\n",
    "print(pp(\"Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯Ù… Ø±Ø§ Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø§Ù… .\"))"
   ],
   "id": "cd23db1c5e50f3f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ú©ØªØ§Ø¨', 'Ø¬Ø¯ÛŒØ¯', 'Ø±Ø§', 'Ø®ÙˆØ§Ù†Ø¯&Ø®ÙˆØ§Ù†']\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ”¹ 8.3 Speed Comparison",
   "id": "8efe1a5102f70a5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T00:30:17.516706Z",
     "start_time": "2025-04-24T00:30:16.856148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs = [sample] * 500  # duplicate sample 500Ã—\n",
    "\n",
    "\n",
    "def time_it(prep, docs):\n",
    "    start = time.perf_counter()\n",
    "    _ = prep.batch(docs)\n",
    "    return time.perf_counter() - start\n",
    "\n",
    "\n",
    "pp_spell = ParsivarPreprocessor(fix_spelling=True, use_stemmer=False)\n",
    "pp_raw = ParsivarPreprocessor(fix_spelling=False, use_stemmer=False)\n",
    "\n",
    "t_spell = time_it(pp_spell, docs)\n",
    "t_raw = time_it(pp_raw, docs)\n",
    "\n",
    "print(f\"With spell-correction : {t_spell:.3f}s\")\n",
    "print(f\"Without correction    : {t_raw:.3f}s\")"
   ],
   "id": "db89bf413d7eae41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With spell-correction : 0.624s\n",
      "Without correction    : 0.034s\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9e77646545c6c3cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
